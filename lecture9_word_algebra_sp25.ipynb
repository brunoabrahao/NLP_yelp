{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Algebra using Word2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Algebra is an AI technique \n",
    "\n",
    "This tutorial features an end-to-end natural language processing pipeline for word algebra, starting with **raw data** and running through **preparing**, **modeling**, **visualizing**, and **analyzing** the data. We'll touch on the following points:\n",
    "1. Overview of the dataset\n",
    "1. Text processing with spaCy\n",
    "1. Automatic phrase modeling\n",
    "1. Topic modeling with LDA\n",
    "1. Visualizing topic models with pyLDAvis\n",
    "1. Word vector models with word2vec\n",
    "1. Visualizing word2vec with t-SNE\n",
    "\n",
    "...and we will learn Python features and packages along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Yelp Dataset\n",
    "[**The Yelp Dataset**](https://www.yelp.com/dataset_challenge/) is a dataset published by the business review service [Yelp](http://yelp.com) for academic research and educational purposes. The dataset is big but and most of the preprocessing and training tasks require several hours to complete.\n",
    "\n",
    "**Note:** To run this code, you'll need to download your own copy of the Yelp dataset. Here's how to get the dataset:\n",
    "1. Go to the Yelp dataset webpage [here](https://www.yelp.com/dataset_challenge/) and download the data.\n",
    "1. The dataset downloads as a compressed .tgz file; uncompress it\n",
    "1. Setup the variable data_directory below with the path where you copied the data to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u26a0\ufe0f Troubleshooting Guide\n",
    "\n",
    "**Before you start**, here are solutions to common issues:\n",
    "\n",
    "### Memory Issues\n",
    "\n",
    "**Problem**: Jupyter kernel crashes or computer freezes\n",
    "- **Solution**: Make sure `RECOMPUTE_DATA = False` (in cell 5)\n",
    "  - This uses pre-computed results instead of processing 4.2M reviews\n",
    "  - Processing from scratch requires 8-16GB RAM\n",
    "\n",
    "**Problem**: \"MemoryError\" when training models\n",
    "- **Solution**: Close other applications\n",
    "- **Alternative**: Reduce `batch_size` in nlp.pipe() calls\n",
    "- **Advanced**: Use a machine with more RAM or cloud notebook (Google Colab)\n",
    "\n",
    "### Runtime Issues\n",
    "\n",
    "**Problem**: Code cells take too long to run\n",
    "- **Expected runtimes** (with `RECOMPUTE_DATA = False`):\n",
    "  - Loading models/data: 1-5 seconds per cell\n",
    "  - t-SNE visualization: 30-60 seconds\n",
    "  - Everything else: < 5 seconds\n",
    "- **If using `RECOMPUTE_DATA = True`**:\n",
    "  - Text preprocessing: 10-20 minutes\n",
    "  - LDA training: 5-10 minutes\n",
    "  - Word2Vec training: 2-3 minutes\n",
    "  - Total: ~30-40 minutes\n",
    "\n",
    "**Problem**: Cells seem stuck or frozen\n",
    "- **Solution**: Look for `[*]` indicator - cell is still running\n",
    "- **Solution**: Check terminal/console for progress messages\n",
    "- **Restart**: Kernel \u2192 Restart & Run All (if truly stuck)\n",
    "\n",
    "### Import Errors\n",
    "\n",
    "**Problem**: `ModuleNotFoundError: No module named 'X'`\n",
    "- **Solution**: Install missing package: `pip install X`\n",
    "- **Common missing packages**: spacy, gensim, pyLDAvis, bokeh, sklearn\n",
    "- **Check**: Run `pip list` to see installed packages\n",
    "\n",
    "**Problem**: `OSError: [E050] Can't find model 'en_core_web_sm'`\n",
    "- **Solution**: Download spaCy model: `python -m spacy download en_core_web_sm`\n",
    "\n",
    "### Data File Issues\n",
    "\n",
    "**Problem**: `FileNotFoundError` for intermediate files\n",
    "- **Solution**: Set `RECOMPUTE_DATA = True` to regenerate files\n",
    "- **Check**: Verify `yelp_dataset/intermediate/` directory exists\n",
    "- **Download**: If missing, download pre-computed files (see README)\n",
    "\n",
    "### Model Quality Issues\n",
    "\n",
    "**Problem**: Word algebra gives weird results\n",
    "- **Possible causes**:\n",
    "  - Word not in vocabulary (appears < 20 times)\n",
    "  - Try more common restaurant terms\n",
    "  - Check spelling and underscores for phrases\n",
    "- **Check**: `'word' in food2vec.wv` to test if word exists\n",
    "\n",
    "**Problem**: LDA topics don't make sense\n",
    "- **This is normal**: Some topics are interpretable, others less so\n",
    "- **Improvement**: Try adjusting `num_topics` parameter\n",
    "- **Remember**: LDA is unsupervised - topics aren't guaranteed to match human intuition\n",
    "\n",
    "### Still Having Issues?\n",
    "\n",
    "- Check that all cells above executed successfully (no error messages)\n",
    "- Restart kernel and run cells in order from top to bottom\n",
    "- Check Python version: Requires Python 3.7+\n",
    "- Check package versions: `pip list | grep -E 'spacy|gensim|pyLDAvis'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = 'yelp_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "The `RECOMPUTE_DATA` variable controls whether to run expensive preprocessing operations or load pre-computed results. Set it to `True` if you want to regenerate all intermediate files from scratch, or `False` to use existing results for faster execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration: Control whether to recompute expensive operations\n",
    "# Set to True to regenerate all intermediate files from scratch\n",
    "# Set to False to use pre-computed results (faster for demos)\n",
    "RECOMPUTE_DATA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we focus on restaurants alone.\n",
    "\n",
    "The data is provided in a handful of files in _.json_ format. We'll be using the following files for our demo:\n",
    "- business.json__ &mdash; _the records for individual businesses_\n",
    "- review.json__ &mdash; _the records for reviews users wrote about businesses_\n",
    "\n",
    "The files are text files (UTF-8) with one _json object_ per line, each one corresponding to an individual data record. Let's take a look at a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"business_id\":\"1SWheh84yJXfytovILXOAQ\",\"name\":\"Arizona Biltmore Golf Club\",\"address\":\"2818 E Camino Acequia Drive\",\"city\":\"Phoenix\",\"state\":\"AZ\",\"postal_code\":\"85016\",\"latitude\":33.5221425,\"longitude\":-112.0184807,\"stars\":3.0,\"review_count\":5,\"is_open\":0,\"attributes\":{\"GoodForKids\":\"False\"},\"categories\":\"Golf, Active Life\",\"hours\":null}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "\n",
    "businesses_filepath = os.path.join(data_directory,\n",
    "                                   'business.json')\n",
    "\n",
    "with codecs.open(businesses_filepath, encoding='utf_8') as f:\n",
    "    first_business_record = f.readline() \n",
    "\n",
    "print(first_business_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The business records consist of _key, value_ pairs containing information about the particular business. A few attributes we'll be interested in for this demo include:\n",
    "- __business\\_id__ &mdash; _unique identifier for businesses_\n",
    "- __categories__ &mdash; _an array containing relevant category values of businesses_\n",
    "\n",
    "The _categories_ attribute is of special interest. This demo will focus on restaurants, which are indicated by the presence of the _Restaurant_ tag in the _categories_ array. In addition, the _categories_ array may contain more detailed information about restaurants, such as the type of food they serve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The review records are stored in a similar manner &mdash; _key, value_ pairs containing information about the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"review_id\":\"Q1sbwvVQXV2734tPgoKj4Q\",\"user_id\":\"hG7b0MtEbXx5QzbzE6C_VA\",\"business_id\":\"ujmEBvifdJM6h6RLv4wQIg\",\"stars\":1.0,\"useful\":6,\"funny\":1,\"cool\":0,\"text\":\"Total bill for this horrible service? Over $8Gs. These crooks actually had the nerve to charge us $69 for 3 pills. I checked online the pills can be had for 19 cents EACH! Avoid Hospital ERs at all costs.\",\"date\":\"2013-05-07 04:34:36\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_json_filepath = os.path.join(data_directory,\n",
    "                                    'review.json')\n",
    "\n",
    "with codecs.open(review_json_filepath, encoding='utf_8') as f:\n",
    "    first_review_record = f.readline()\n",
    "    \n",
    "print(first_review_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few attributes of note on the review records:\n",
    "- __business\\_id__ &mdash; _indicates which business the review is about_\n",
    "- __text__ &mdash; _the natural language text the user wrote_\n",
    "\n",
    "The _text_ attribute will be our focus today!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_json_ is a handy file format for data interchange, but it's typically not the most usable for any sort of modeling work. Let's do a bit more data preparation to get our data in a more usable format. Our next code block will do the following:\n",
    "1. Read in each business record and convert it to a Python `dict`\n",
    "2. Filter out business records that aren't about restaurants (i.e., not in the \"Restaurant\" category)\n",
    "3. Create a `frozenset` of the business IDs for restaurants, which we'll use in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59,853 restaurants in the dataset.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "restaurant_ids = set()\n",
    "\n",
    "# open the businesses file\n",
    "with codecs.open(businesses_filepath, encoding='utf_8') as f:\n",
    "    \n",
    "    # iterate through each line (json record) in the file\n",
    "    for business_json in f:\n",
    "        \n",
    "        # convert the json record to a Python dict\n",
    "        business = json.loads(business_json)\n",
    "        # if this business is not a restaurant, skip to the next one\n",
    "        if business['categories'] is not None and 'Restaurants' not in business['categories']:\n",
    "            continue\n",
    "        # add the restaurant business id to our restaurant_ids set\n",
    "        restaurant_ids.add(business['business_id'])\n",
    "\n",
    "# turn restaurant_ids into a frozenset, as we don't need to change it anymore\n",
    "restaurant_ids = frozenset(restaurant_ids)\n",
    "\n",
    "# print the number of unique restaurant ids in the dataset\n",
    "print(f'{len(restaurant_ids):,} restaurants in the dataset.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a new file that contains only the text from reviews about restaurants, with one review per line in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_directory = os.path.join(data_directory, 'intermediate')\n",
    "\n",
    "if not os.path.exists(intermediate_directory):\n",
    "    os.makedirs(intermediate_directory)\n",
    "\n",
    "review_txt_filepath = os.path.join(intermediate_directory,\n",
    "                                   'review_text_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from 4,203,821 restaurant reviews in the txt file.\n",
      "CPU times: user 248 \u03bcs, sys: 2.95 ms, total: 3.19 ms\n",
      "Wall time: 528 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if RECOMPUTE_DATA:\n",
    "    \n",
    "    review_count = 0\n",
    "\n",
    "    # create & open a new file in write mode\n",
    "    with codecs.open(review_txt_filepath, 'w', encoding='utf_8') as review_txt_file:\n",
    "\n",
    "        # open the existing review json file\n",
    "        with codecs.open(review_json_filepath, encoding='utf_8') as review_json_file:\n",
    "\n",
    "            # loop through all reviews in the existing file and convert to dict\n",
    "            for review_json in review_json_file:\n",
    "                review = json.loads(review_json)\n",
    "\n",
    "                # if this review is not about a restaurant, skip to the next one\n",
    "                if review['business_id'] not in restaurant_ids:\n",
    "                    continue\n",
    "\n",
    "                # write the restaurant review as a line in the new file\n",
    "                # escape newline characters in the original review text\n",
    "                review_txt_file.write(review['text'].replace('\\n', '\\\\n') + '\\n')\n",
    "                review_count += 1\n",
    "\n",
    "    print(f'Text from {review_count:,} restaurant reviews written to the new txt file.')\n",
    "    \n",
    "else:\n",
    "    # Fast line counting: Use shell command wc -l (much faster than Python iteration)\n",
    "    import subprocess\n",
    "    \n",
    "    try:\n",
    "        # Use wc -l which is optimized for counting lines (10-100x faster)\n",
    "        result = subprocess.run(['wc', '-l', review_txt_filepath], \n",
    "                              capture_output=True, text=True, check=True)\n",
    "        review_count = int(result.stdout.split()[0])\n",
    "        print(f'Text from {review_count:,} restaurant reviews in the txt file.')\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        # Fallback to Python counting if wc is not available (Windows)\n",
    "        # Use buffered reading for better performance\n",
    "        def count_lines_fast(filename):\n",
    "            with open(filename, 'rb') as f:\n",
    "                return sum(1 for _ in f)\n",
    "        \n",
    "        review_count = count_lines_fast(review_txt_filepath)\n",
    "        print(f'Text from {review_count:,} restaurant reviews in the txt file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd0d Part 2: spaCy - Industrial-Strength NLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### \ud83c\udfaf Learning Objectives:\n",
    "- Master text preprocessing fundamentals\n",
    "- Learn tokenization, lemmatization, and NER\n",
    "- Understand how spaCy processes text efficiently\n",
    "- See token attributes like part-of-speech and probabilities\n",
    "\n",
    "**Time:** ~10 minutes | **Key Concept:** Text normalization and linguistic analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spaCy](https://s3.amazonaws.com/skipgram-images/spaCy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**spaCy**](https://spacy.io) is an industrial-strength natural language processing (_NLP_) library for Python. spaCy's goal is to take recent advancements in natural language processing out of research papers and put them in the hands of users to build production software.\n",
    "\n",
    "spaCy handles many tasks commonly associated with building an end-to-end natural language processing pipeline:\n",
    "- Tokenization\n",
    "- Text normalization, such as lowercasing, stemming/lemmatization\n",
    "- Part-of-speech tagging\n",
    "- Syntactic dependency parsing\n",
    "- Sentence boundary detection\n",
    "- Named entity recognition and annotation\n",
    "\n",
    "In the \"batteries included\" Python tradition, spaCy contains built-in data and models which you can use out-of-the-box for processing general-purpose English language text:\n",
    "- Large English vocabulary, including stopword lists\n",
    "- Token \"probabilities\"\n",
    "- Word vectors\n",
    "\n",
    "spaCy is written in optimized Cython, which means it's _fast_. According to a few independent sources, it's the fastest syntactic parser available in any language. Key pieces of the spaCy parsing pipeline are written in pure C, enabling efficient multithreading (i.e., spaCy can release the _GIL_).\n",
    "\n",
    "Spacy can be installed via your Python Anaconda distribution\n",
    "\n",
    "You need to download the English model\n",
    "\n",
    "1) Open your terminal prompt and type\n",
    "\n",
    "python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "\n",
    "# WHY en_core_web_sm (small model) instead of en_core_web_lg (large):\n",
    "# - sm: 12MB download, includes tokenizer, POS tagger, lemmatizer, NER\n",
    "# - lg: 560MB download, adds word vectors (but we're training our own with Word2Vec!)\n",
    "# - For this tutorial, we only need tokenization and lemmatization\n",
    "# - The large model's pre-trained vectors would be wasted\n",
    "# - Result: Faster loading, less disk space, same functionality\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#nlp = spacy.load('en_core_web_lg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab a sample review to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love chinese food and I love mexican food. What can go wrong? A couple of things. First things first, this place is more of a \"rice bowl\" kind of place. I thought it was going to be more diverse as far as the menu goes, but its mainly rice bowls you get with different kinds of meats. The ordering was a little confusing at first, but one of the employees helped us out and I got the 2-item bowl and got the jade chicken and hengrenade chicken with all rice(jerk). I also ordered a jade chicken quesadilla on the side.\n",
      "\n",
      "I'm gonna admit, this place looks kinda dirty. I don't think Arizona uses those health department letter grade system like California does, but if I were to just judge by how it looked inside, i'd give it a \"C\" grade lol. We waited for about 15 minutes or so and finally got our food. We took it to go and ate at our hotel room. \n",
      "\n",
      "Mmmm... the food was just alright. The jade chicken was nothing special. It tasted like any generic chinese fast food orange chicken/sesame chicken variant. The hengrenade chicken, although was the less spicier version of the jerk chicken, was still pretty spicy for me. Just be warned the jerk chicken is super spicy. If you aren't sure, ask for a sample at the restaurant before ordering, but it was way too spicy for me. \n",
      "\n",
      "The jade chicken quesadilla was decent, but nothing special. Just imagine orange chicken in between a tortilla and cheese. A friend of mine ordered a jade chicken burrito and we were confused when we pulled it out of the bag because it was literally the size of Mcdonald's apple pie. If you order the burrito, be warned that it's a burrito for gnomes and smurfs, but he said it was tasty. \n",
      "\n",
      "They provide a snicker doodle sugar cookie for each meal and it was decent, again nothing special. \n",
      "\n",
      "Not gonna lie, the next day my stomach felt like a little mexican dude and chinese dude were wrestling and throwing molotov cocktails inside. I used the bathroom like 5 times. I don't recommend eating this place if you have a lot to do the next day.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with codecs.open(review_txt_filepath, encoding='utf_8') as f:\n",
    "    sample_review = list(it.islice(f, 10, 11))[0]\n",
    "    sample_review = sample_review.replace('\\\\n', '\\n')\n",
    "        \n",
    "print(sample_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hand the review text to spaCy, and be prepared to wait..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 99.3 ms, sys: 7.92 ms, total: 107 ms\n",
      "Wall time: 106 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parsed_review = nlp(sample_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...1/20th of a second or so. Let's take a look at what we got during that time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love chinese food and I love mexican food. What can go wrong? A couple of things. First things first, this place is more of a \"rice bowl\" kind of place. I thought it was going to be more diverse as far as the menu goes, but its mainly rice bowls you get with different kinds of meats. The ordering was a little confusing at first, but one of the employees helped us out and I got the 2-item bowl and got the jade chicken and hengrenade chicken with all rice(jerk). I also ordered a jade chicken quesadilla on the side.\n",
      "\n",
      "I'm gonna admit, this place looks kinda dirty. I don't think Arizona uses those health department letter grade system like California does, but if I were to just judge by how it looked inside, i'd give it a \"C\" grade lol. We waited for about 15 minutes or so and finally got our food. We took it to go and ate at our hotel room. \n",
      "\n",
      "Mmmm... the food was just alright. The jade chicken was nothing special. It tasted like any generic chinese fast food orange chicken/sesame chicken variant. The hengrenade chicken, although was the less spicier version of the jerk chicken, was still pretty spicy for me. Just be warned the jerk chicken is super spicy. If you aren't sure, ask for a sample at the restaurant before ordering, but it was way too spicy for me. \n",
      "\n",
      "The jade chicken quesadilla was decent, but nothing special. Just imagine orange chicken in between a tortilla and cheese. A friend of mine ordered a jade chicken burrito and we were confused when we pulled it out of the bag because it was literally the size of Mcdonald's apple pie. If you order the burrito, be warned that it's a burrito for gnomes and smurfs, but he said it was tasty. \n",
      "\n",
      "They provide a snicker doodle sugar cookie for each meal and it was decent, again nothing special. \n",
      "\n",
      "Not gonna lie, the next day my stomach felt like a little mexican dude and chinese dude were wrestling and throwing molotov cocktails inside. I used the bathroom like 5 times. I don't recommend eating this place if you have a lot to do the next day.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(parsed_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks the same! What happened under the hood?\n",
    "\n",
    "What about sentence detection and segmentation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "I love chinese food and I love mexican food.\n",
      "\n",
      "Sentence 2:\n",
      "What can go wrong?\n",
      "\n",
      "Sentence 3:\n",
      "A couple of things.\n",
      "\n",
      "Sentence 4:\n",
      "First things first, this place is more of a \"rice bowl\" kind of place.\n",
      "\n",
      "Sentence 5:\n",
      "I thought it was going to be more diverse as far as the menu goes, but its mainly rice bowls you get with different kinds of meats.\n",
      "\n",
      "Sentence 6:\n",
      "The ordering was a little confusing at first, but one of the employees helped us out and I got the 2-item bowl and got the jade chicken and hengrenade chicken with all rice(jerk).\n",
      "\n",
      "Sentence 7:\n",
      "I also ordered a jade chicken quesadilla on the side.\n",
      "\n",
      "\n",
      "\n",
      "Sentence 8:\n",
      "I'm gonna admit, this place looks kinda dirty.\n",
      "\n",
      "Sentence 9:\n",
      "I don't think Arizona uses those health department letter grade system like California does, but if I were to just judge by how it looked inside, i'd give it a \"C\" grade lol.\n",
      "\n",
      "Sentence 10:\n",
      "We waited for about 15 minutes or so and finally got our food.\n",
      "\n",
      "Sentence 11:\n",
      "We took it to go and ate at our hotel room. \n",
      "\n",
      "\n",
      "\n",
      "Sentence 12:\n",
      "Mmmm... the food was just alright.\n",
      "\n",
      "Sentence 13:\n",
      "The jade chicken was nothing special.\n",
      "\n",
      "Sentence 14:\n",
      "It tasted like any generic chinese fast food orange chicken/sesame chicken variant.\n",
      "\n",
      "Sentence 15:\n",
      "The hengrenade chicken, although was the less spicier version of the jerk chicken, was still pretty spicy for me.\n",
      "\n",
      "Sentence 16:\n",
      "Just be warned the jerk chicken is super spicy.\n",
      "\n",
      "Sentence 17:\n",
      "If you aren't sure, ask for a sample at the restaurant before ordering, but it was way too spicy for me. \n",
      "\n",
      "\n",
      "\n",
      "Sentence 18:\n",
      "The jade chicken quesadilla was decent, but nothing special.\n",
      "\n",
      "Sentence 19:\n",
      "Just imagine orange chicken in between a tortilla and cheese.\n",
      "\n",
      "Sentence 20:\n",
      "A friend of mine ordered a jade chicken burrito and we were confused when we pulled it out of the bag because it was literally the size of Mcdonald's apple pie.\n",
      "\n",
      "Sentence 21:\n",
      "If you order the burrito, be warned that it's a burrito for gnomes and smurfs, but he said it was tasty. \n",
      "\n",
      "\n",
      "\n",
      "Sentence 22:\n",
      "They provide a snicker doodle sugar cookie for each meal and it was decent, again nothing special. \n",
      "\n",
      "\n",
      "\n",
      "Sentence 23:\n",
      "Not gonna lie, the next day my stomach felt like a little mexican dude and chinese dude were wrestling and throwing molotov cocktails inside.\n",
      "\n",
      "Sentence 24:\n",
      "I used the bathroom like 5 times.\n",
      "\n",
      "Sentence 25:\n",
      "I don't recommend eating this place if you have a lot to do the next day.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, sentence in enumerate(parsed_review.sents):\n",
    "    print('Sentence {}:'.format(num + 1))\n",
    "    print(sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about named entity detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 1: chinese - NORP\n",
      "\n",
      "Entity 2: mexican - NORP\n",
      "\n",
      "Entity 3: First - ORDINAL\n",
      "\n",
      "Entity 4: first - ORDINAL\n",
      "\n",
      "Entity 5: first - ORDINAL\n",
      "\n",
      "Entity 6: 2 - CARDINAL\n",
      "\n",
      "Entity 7: Arizona - GPE\n",
      "\n",
      "Entity 8: California - GPE\n",
      "\n",
      "Entity 9: about 15 minutes - TIME\n",
      "\n",
      "Entity 10: Mmmm - GPE\n",
      "\n",
      "Entity 11: chinese - NORP\n",
      "\n",
      "Entity 12: Mcdonald - ORG\n",
      "\n",
      "Entity 13: the next day - DATE\n",
      "\n",
      "Entity 14: mexican - NORP\n",
      "\n",
      "Entity 15: chinese - NORP\n",
      "\n",
      "Entity 16: 5 - CARDINAL\n",
      "\n",
      "Entity 17: the next day - DATE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, entity in enumerate(parsed_review.ents):\n",
    "    print('Entity {}:'.format(num + 1), entity, '-', entity.label_)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about part of speech tagging?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "7af7185f-5251-45f9-8561-d6bf878688f8",
       "rows": [
        [
         "PRON",
         "I"
        ],
        [
         "VERB",
         "love"
        ],
        [
         "ADJ",
         "chinese"
        ],
        [
         "NOUN",
         "food"
        ],
        [
         "CCONJ",
         "and"
        ],
        [
         "PRON",
         "I"
        ],
        [
         "VERB",
         "love"
        ],
        [
         "ADJ",
         "mexican"
        ],
        [
         "NOUN",
         "food"
        ],
        [
         "PUNCT",
         "."
        ],
        [
         "PRON",
         "What"
        ],
        [
         "AUX",
         "can"
        ],
        [
         "VERB",
         "go"
        ],
        [
         "ADJ",
         "wrong"
        ],
        [
         "PUNCT",
         "?"
        ],
        [
         "DET",
         "A"
        ],
        [
         "NOUN",
         "couple"
        ],
        [
         "ADP",
         "of"
        ],
        [
         "NOUN",
         "things"
        ],
        [
         "PUNCT",
         "."
        ],
        [
         "ADJ",
         "First"
        ],
        [
         "NOUN",
         "things"
        ],
        [
         "ADV",
         "first"
        ],
        [
         "PUNCT",
         ","
        ],
        [
         "DET",
         "this"
        ],
        [
         "NOUN",
         "place"
        ],
        [
         "AUX",
         "is"
        ],
        [
         "ADJ",
         "more"
        ],
        [
         "ADP",
         "of"
        ],
        [
         "DET",
         "a"
        ],
        [
         "PUNCT",
         "\""
        ],
        [
         "NOUN",
         "rice"
        ],
        [
         "NOUN",
         "bowl"
        ],
        [
         "PUNCT",
         "\""
        ],
        [
         "NOUN",
         "kind"
        ],
        [
         "ADP",
         "of"
        ],
        [
         "NOUN",
         "place"
        ],
        [
         "PUNCT",
         "."
        ],
        [
         "PRON",
         "I"
        ],
        [
         "VERB",
         "thought"
        ],
        [
         "PRON",
         "it"
        ],
        [
         "AUX",
         "was"
        ],
        [
         "VERB",
         "going"
        ],
        [
         "PART",
         "to"
        ],
        [
         "AUX",
         "be"
        ],
        [
         "ADV",
         "more"
        ],
        [
         "ADJ",
         "diverse"
        ],
        [
         "ADV",
         "as"
        ],
        [
         "ADV",
         "far"
        ],
        [
         "SCONJ",
         "as"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 439
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCONJ</th>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>next</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUNCT</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPACE</th>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>439 rows \u00d7 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "PRON         I\n",
       "VERB      love\n",
       "ADJ    chinese\n",
       "NOUN      food\n",
       "CCONJ      and\n",
       "...        ...\n",
       "DET        the\n",
       "ADJ       next\n",
       "NOUN       day\n",
       "PUNCT        .\n",
       "SPACE       \\n\n",
       "\n",
       "[439 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_text = [token.orth_ for token in parsed_review]\n",
    "token_pos = [token.pos_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(token_text, token_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about text normalization, like stemming/lemmatization and shape analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "(None, None)",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "0",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "f222519e-8a89-4fe8-8087-2fd85652cf83",
       "rows": [
        [
         "('I', 'X')",
         "I"
        ],
        [
         "('love', 'xxxx')",
         "love"
        ],
        [
         "('chinese', 'xxxx')",
         "chinese"
        ],
        [
         "('food', 'xxxx')",
         "food"
        ],
        [
         "('and', 'xxx')",
         "and"
        ],
        [
         "('I', 'X')",
         "I"
        ],
        [
         "('love', 'xxxx')",
         "love"
        ],
        [
         "('mexican', 'xxxx')",
         "mexican"
        ],
        [
         "('food', 'xxxx')",
         "food"
        ],
        [
         "('.', '.')",
         "."
        ],
        [
         "('what', 'Xxxx')",
         "What"
        ],
        [
         "('can', 'xxx')",
         "can"
        ],
        [
         "('go', 'xx')",
         "go"
        ],
        [
         "('wrong', 'xxxx')",
         "wrong"
        ],
        [
         "('?', '?')",
         "?"
        ],
        [
         "('a', 'X')",
         "A"
        ],
        [
         "('couple', 'xxxx')",
         "couple"
        ],
        [
         "('of', 'xx')",
         "of"
        ],
        [
         "('thing', 'xxxx')",
         "things"
        ],
        [
         "('.', '.')",
         "."
        ],
        [
         "('first', 'Xxxxx')",
         "First"
        ],
        [
         "('thing', 'xxxx')",
         "things"
        ],
        [
         "('first', 'xxxx')",
         "first"
        ],
        [
         "(',', ',')",
         ","
        ],
        [
         "('this', 'xxxx')",
         "this"
        ],
        [
         "('place', 'xxxx')",
         "place"
        ],
        [
         "('be', 'xx')",
         "is"
        ],
        [
         "('more', 'xxxx')",
         "more"
        ],
        [
         "('of', 'xx')",
         "of"
        ],
        [
         "('a', 'x')",
         "a"
        ],
        [
         "('\"', '\"')",
         "\""
        ],
        [
         "('rice', 'xxxx')",
         "rice"
        ],
        [
         "('bowl', 'xxxx')",
         "bowl"
        ],
        [
         "('\"', '\"')",
         "\""
        ],
        [
         "('kind', 'xxxx')",
         "kind"
        ],
        [
         "('of', 'xx')",
         "of"
        ],
        [
         "('place', 'xxxx')",
         "place"
        ],
        [
         "('.', '.')",
         "."
        ],
        [
         "('I', 'X')",
         "I"
        ],
        [
         "('think', 'xxxx')",
         "thought"
        ],
        [
         "('it', 'xx')",
         "it"
        ],
        [
         "('be', 'xxx')",
         "was"
        ],
        [
         "('go', 'xxxx')",
         "going"
        ],
        [
         "('to', 'xx')",
         "to"
        ],
        [
         "('be', 'xx')",
         "be"
        ],
        [
         "('more', 'xxxx')",
         "more"
        ],
        [
         "('diverse', 'xxxx')",
         "diverse"
        ],
        [
         "('as', 'xx')",
         "as"
        ],
        [
         "('far', 'xxx')",
         "far"
        ],
        [
         "('as', 'xx')",
         "as"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 439
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <th>X</th>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <th>xxxx</th>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chinese</th>\n",
       "      <th>xxxx</th>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <th>xxxx</th>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <th>xxx</th>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <th>xxx</th>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>next</th>\n",
       "      <th>xxxx</th>\n",
       "      <td>next</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <th>xxx</th>\n",
       "      <td>day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <th>.</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\n</th>\n",
       "      <th>\\n</th>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>439 rows \u00d7 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0\n",
       "I       X           I\n",
       "love    xxxx     love\n",
       "chinese xxxx  chinese\n",
       "food    xxxx     food\n",
       "and     xxx       and\n",
       "...               ...\n",
       "the     xxx       the\n",
       "next    xxxx     next\n",
       "day     xxx       day\n",
       ".       .           .\n",
       "\\n      \\n         \\n\n",
       "\n",
       "[439 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_lemma = [token.lemma_ for token in parsed_review]\n",
    "token_shape = [token.shape_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(token_text, [token_lemma, token_shape])\n",
    "             #columns=['token_lemma', 'token_shape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about token-level entity analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "(None, None)",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "0",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "ba173eac-a5fb-4356-841e-fa96876136a1",
       "rows": [
        [
         "('', 'O')",
         "I"
        ],
        [
         "('', 'O')",
         "love"
        ],
        [
         "('NORP', 'B')",
         "chinese"
        ],
        [
         "('', 'O')",
         "food"
        ],
        [
         "('', 'O')",
         "and"
        ],
        [
         "('', 'O')",
         "I"
        ],
        [
         "('', 'O')",
         "love"
        ],
        [
         "('NORP', 'B')",
         "mexican"
        ],
        [
         "('', 'O')",
         "food"
        ],
        [
         "('', 'O')",
         "."
        ],
        [
         "('', 'O')",
         "What"
        ],
        [
         "('', 'O')",
         "can"
        ],
        [
         "('', 'O')",
         "go"
        ],
        [
         "('', 'O')",
         "wrong"
        ],
        [
         "('', 'O')",
         "?"
        ],
        [
         "('', 'O')",
         "A"
        ],
        [
         "('', 'O')",
         "couple"
        ],
        [
         "('', 'O')",
         "of"
        ],
        [
         "('', 'O')",
         "things"
        ],
        [
         "('', 'O')",
         "."
        ],
        [
         "('ORDINAL', 'B')",
         "First"
        ],
        [
         "('', 'O')",
         "things"
        ],
        [
         "('ORDINAL', 'B')",
         "first"
        ],
        [
         "('', 'O')",
         ","
        ],
        [
         "('', 'O')",
         "this"
        ],
        [
         "('', 'O')",
         "place"
        ],
        [
         "('', 'O')",
         "is"
        ],
        [
         "('', 'O')",
         "more"
        ],
        [
         "('', 'O')",
         "of"
        ],
        [
         "('', 'O')",
         "a"
        ],
        [
         "('', 'O')",
         "\""
        ],
        [
         "('', 'O')",
         "rice"
        ],
        [
         "('', 'O')",
         "bowl"
        ],
        [
         "('', 'O')",
         "\""
        ],
        [
         "('', 'O')",
         "kind"
        ],
        [
         "('', 'O')",
         "of"
        ],
        [
         "('', 'O')",
         "place"
        ],
        [
         "('', 'O')",
         "."
        ],
        [
         "('', 'O')",
         "I"
        ],
        [
         "('', 'O')",
         "thought"
        ],
        [
         "('', 'O')",
         "it"
        ],
        [
         "('', 'O')",
         "was"
        ],
        [
         "('', 'O')",
         "going"
        ],
        [
         "('', 'O')",
         "to"
        ],
        [
         "('', 'O')",
         "be"
        ],
        [
         "('', 'O')",
         "more"
        ],
        [
         "('', 'O')",
         "diverse"
        ],
        [
         "('', 'O')",
         "as"
        ],
        [
         "('', 'O')",
         "far"
        ],
        [
         "('', 'O')",
         "as"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 439
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\"></th>\n",
       "      <th>O</th>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NORP</th>\n",
       "      <th>B</th>\n",
       "      <td>chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\"></th>\n",
       "      <th>O</th>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DATE</th>\n",
       "      <th>B</th>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>next</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\"></th>\n",
       "      <th>O</th>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>439 rows \u00d7 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "     O        I\n",
       "     O     love\n",
       "NORP B  chinese\n",
       "     O     food\n",
       "     O      and\n",
       "...         ...\n",
       "DATE B      the\n",
       "     I     next\n",
       "     I      day\n",
       "     O        .\n",
       "     O       \\n\n",
       "\n",
       "[439 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_entity_type = [token.ent_type_ for token in parsed_review]\n",
    "token_entity_iob = [token.ent_iob_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(token_text, [token_entity_type, token_entity_iob])#,\n",
    "             #columns=['token_text', 'entity_type', 'inside_outside_begin'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about a variety of other token-level attributes, such as the relative frequency of tokens, and whether or not a token matches any of these categories?\n",
    "- stopword\n",
    "- punctuation\n",
    "- whitespace\n",
    "- represents a number\n",
    "- whether or not the token is included in spaCy's default vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "stop?",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "punctuation?",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "whitespace?",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "number?",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "out of vocab.?",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "048b0f0d-8b0d-445b-9c36-188eb3ad0b36",
       "rows": [
        [
         "0",
         "I",
         "11",
         "Yes",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "1",
         "love",
         "2",
         "",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "2",
         "chinese",
         "3",
         "",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "3",
         "food",
         "5",
         "",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "4",
         "and",
         "12",
         "Yes",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "5",
         "I",
         "11",
         "Yes",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "6",
         "love",
         "2",
         "",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "7",
         "mexican",
         "2",
         "",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "8",
         "food",
         "5",
         "",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "9",
         ".",
         "24",
         "",
         "Yes",
         "",
         "",
         "Yes"
        ],
        [
         "10",
         "What",
         "1",
         "Yes",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "11",
         "can",
         "1",
         "Yes",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "12",
         "go",
         "2",
         "Yes",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "13",
         "wrong",
         "1",
         "",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "14",
         "?",
         "1",
         "",
         "Yes",
         "",
         "",
         "Yes"
        ],
        [
         "15",
         "A",
         "13",
         "Yes",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "16",
         "couple",
         "1",
         "",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "17",
         "of",
         "9",
         "Yes",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "18",
         "things",
         "2",
         "",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "19",
         ".",
         "24",
         "",
         "Yes",
         "",
         "",
         "Yes"
        ],
        [
         "20",
         "First",
         "3",
         "Yes",
         "",
         "",
         "Yes",
         "Yes"
        ],
        [
         "21",
         "things",
         "2",
         "",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "22",
         "first",
         "3",
         "Yes",
         "",
         "",
         "Yes",
         "Yes"
        ],
        [
         "23",
         ",",
         "15",
         "",
         "Yes",
         "",
         "",
         "Yes"
        ],
        [
         "24",
         "this",
         "3",
         "Yes",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "25",
         "place",
         "4",
         "",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "26",
         "is",
         "2",
         "Yes",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "27",
         "more",
         "2",
         "Yes",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "28",
         "of",
         "9",
         "Yes",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "29",
         "a",
         "13",
         "Yes",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "30",
         "\"",
         "4",
         "",
         "Yes",
         "",
         "",
         "Yes"
        ],
        [
         "31",
         "rice",
         "2",
         "",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "32",
         "bowl",
         "2",
         "",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "33",
         "\"",
         "4",
         "",
         "Yes",
         "",
         "",
         "Yes"
        ],
        [
         "34",
         "kind",
         "1",
         "",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "35",
         "of",
         "9",
         "Yes",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "36",
         "place",
         "4",
         "",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "37",
         ".",
         "24",
         "",
         "Yes",
         "",
         "",
         "Yes"
        ],
        [
         "38",
         "I",
         "11",
         "Yes",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "39",
         "thought",
         "1",
         "",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "40",
         "it",
         "11",
         "Yes",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "41",
         "was",
         "11",
         "Yes",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "42",
         "going",
         "1",
         "",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "43",
         "to",
         "4",
         "Yes",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "44",
         "be",
         "3",
         "Yes",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "45",
         "more",
         "2",
         "Yes",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "46",
         "diverse",
         "1",
         "",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "47",
         "as",
         "2",
         "Yes",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "48",
         "far",
         "1",
         "",
         "",
         "",
         "",
         "Yes"
        ],
        [
         "49",
         "as",
         "2",
         "Yes",
         "",
         "",
         "",
         "Yes"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 439
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>count</th>\n",
       "      <th>stop?</th>\n",
       "      <th>punctuation?</th>\n",
       "      <th>whitespace?</th>\n",
       "      <th>number?</th>\n",
       "      <th>out of vocab.?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>11</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chinese</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>food</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and</td>\n",
       "      <td>12</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>the</td>\n",
       "      <td>20</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>next</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>day</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>.</td>\n",
       "      <td>24</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>\\n</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>439 rows \u00d7 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        text  count stop? punctuation? whitespace? number? out of vocab.?\n",
       "0          I     11   Yes                                             Yes\n",
       "1       love      2                                                   Yes\n",
       "2    chinese      3                                                   Yes\n",
       "3       food      5                                                   Yes\n",
       "4        and     12   Yes                                             Yes\n",
       "..       ...    ...   ...          ...         ...     ...            ...\n",
       "434      the     20   Yes                                             Yes\n",
       "435     next      2   Yes                                             Yes\n",
       "436      day      2                                                   Yes\n",
       "437        .     24                Yes                                Yes\n",
       "438       \\n      1                            Yes                    Yes\n",
       "\n",
       "[439 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count how many times each word appears in this review\n",
    "word_counts = Counter(token.text.lower() for token in parsed_review)\n",
    "\n",
    "token_attributes = [(token.text,  # Fixed: was token.orth_ which returns hash IDs\n",
    "                     word_counts[token.text.lower()],  # Actual word count in this review\n",
    "                     'Yes' if token.is_stop else '',  # Convert to string directly\n",
    "                     'Yes' if token.is_punct else '',\n",
    "                     'Yes' if token.is_space else '',\n",
    "                     'Yes' if token.like_num else '',\n",
    "                     'Yes' if token.is_oov else '')\n",
    "                    for token in parsed_review]\n",
    "\n",
    "df = pd.DataFrame(token_attributes,\n",
    "                  columns=['text',\n",
    "                           'count',  # Shows how many times the word appears\n",
    "                           'stop?',\n",
    "                           'punctuation?',\n",
    "                           'whitespace?',\n",
    "                           'number?',\n",
    "                           'out of vocab.?'])\n",
    "                                               \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the text you'd like to process is general-purpose English language text (i.e., not domain-specific, like medical literature), spaCy is ready to use out-of-the-box.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### \u2705 Key Takeaways - spaCy:\n",
    "- **Tokenization:** Splits text into individual words and punctuation\n",
    "- **Lemmatization:** Converts words to base form (running \u2192 run)\n",
    "- **POS Tagging:** Identifies parts of speech (noun, verb, etc.)\n",
    "- **NER:** Finds named entities (people, places, organizations)\n",
    "- **Fast & Efficient:** Processes millions of tokens quickly using optimized C code\n",
    "\n",
    "\ud83d\udca1 **Why it matters:** Clean, normalized text is essential for all downstream NLP tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd17 Part 3: Phrase Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### \ud83c\udfaf Learning Objectives:\n",
    "- Understand how multi-word expressions are detected\n",
    "- Learn bigram and trigram modeling\n",
    "- See statistical measures for phrase detection\n",
    "- Apply phrase models to transform text\n",
    "\n",
    "**Time:** ~15 minutes | **Key Concept:** Identifying and joining multi-word concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Phrase modeling_ is another approach to learning combinations of tokens that together represent meaningful multi-word concepts. We can develop phrase models by looping over the the words in our reviews and looking for words that _co-occur_ (i.e., appear one after another) together much more frequently than you would expect them to by random chance. The formula our phrase models will use to determine whether two tokens $A$ and $B$ constitute a phrase is:\n",
    "\n",
    "$$\\frac{count(A\\ B) - count_{min}}{count(A) * count(B)} * N > threshold$$\n",
    "\n",
    "...where:\n",
    "* $count(A)$ is the number of times token $A$ appears in the corpus\n",
    "* $count(B)$ is the number of times token $B$ appears in the corpus\n",
    "* $count(A\\ B)$ is the number of times the tokens $A\\ B$ appear in the corpus *in order*\n",
    "* $N$ is the total size of the corpus vocabulary\n",
    "* $count_{min}$ is a user-defined parameter to ensure that accepted phrases occur a minimum number of times\n",
    "* $threshold$ is a user-defined parameter to control how strong of a relationship between two tokens the model requires before accepting them as a phrase\n",
    "\n",
    "Once our phrase model has been trained on our corpus, we can apply it to new text. When our model encounters two tokens in new text that identifies as a phrase, it will merge the two into a single new token.\n",
    "\n",
    "Phrase modeling is superficially similar to named entity detection in that you would expect named entities to become phrases in the model (so _new york_ would become *new\\_york*). But you would also expect multi-word expressions that represent common concepts, but aren't specifically named entities (such as _happy hour_) to also become phrases in the model.\n",
    "\n",
    "We turn to the [**gensim**](https://radimrehurek.com/gensim/index.html) library to help us with phrase modeling &mdash; the [**Phrases**](https://radimrehurek.com/gensim/models/phrases.html) class in particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83c\udf0d Real-World Applications: Phrase Detection\n",
    "\n",
    "Automatic phrase detection has important real-world uses:\n",
    "\n",
    "**Search Engines:**\n",
    "- **Google**: Understands \"ice cream\" as a unit, not separate \"ice\" and \"cream\" searches\n",
    "- **E-commerce**: Searching \"running shoes\" on Amazon treats it as a single concept\n",
    "  - Without phrase detection: might return ice skates (shoes + running keywords separately)\n",
    "\n",
    "**Voice Assistants:**\n",
    "- **Alexa/Siri**: Must recognize multi-word commands correctly\n",
    "  - \"Turn off kitchen lights\" - \"kitchen_lights\" is one entity\n",
    "  - \"Play happy birthday\" - \"happy_birthday\" is one song concept\n",
    "\n",
    "**Machine Translation:**\n",
    "- **Google Translate**: Translates phrases as units for better accuracy\n",
    "  - \"hot dog\" \u2192 German \"Hotdog\" (not \"hei\u00dfer Hund\" = \"hot dog\" literally)\n",
    "  - \"New York\" stays as one entity, not \"New\" + \"York\" separately\n",
    "\n",
    "**Named Entity Recognition:**\n",
    "- **News Analysis**: Extract company names, person names, locations\n",
    "  - \"Bank of America\", \"San Francisco\", \"Bernie Sanders\"\n",
    "  - These must be recognized as single entities\n",
    "\n",
    "**For restaurant reviews:**\n",
    "- **Menu Item Extraction**: Identify signature dishes\n",
    "  - \"chocolate_lava_cake\", \"buffalo_chicken_wings\", \"caesar_salad\"\n",
    "- **Sentiment Analysis**: Better understand what customers like/dislike\n",
    "  - \"amazing happy_hour specials\" - \"happy_hour\" is the praised concept\n",
    "- **Competitive Analysis**: Track trending menu items across restaurants\n",
    "\n",
    "Phrase detection is a foundational step that improves all downstream NLP tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we're performing phrase modeling, we'll be doing some iterative data transformation at the same time. Our roadmap for data preparation includes:\n",
    "\n",
    "1. Segment text of complete reviews into sentences & normalize text\n",
    "1. First-order phrase modeling $\\rightarrow$ _apply first-order phrase model to transform sentences_\n",
    "1. Second-order phrase modeling $\\rightarrow$ _apply second-order phrase model to transform sentences_\n",
    "1. Apply text normalization and second-order phrase model to text of complete reviews\n",
    "\n",
    "We'll use this transformed data as the input for some higher-level modeling approaches in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define a few helper functions that we'll use for text normalization. In particular, the `lemmatized_sentence_corpus` generator function will use spaCy to:\n",
    "- Iterate over the 4M reviews in the `review_txt_all.txt` we created before\n",
    "- Segment the reviews into individual sentences\n",
    "- Remove punctuation and excess whitespace\n",
    "- Lemmatize the text\n",
    "\n",
    "... and do so efficiently in parallel, thanks to spaCy's `nlp.pipe()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \n",
    "    WHY: We remove these because they don't carry semantic meaning\n",
    "    and would create noise in our topic and word vector models.\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in reviews from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \n",
    "    WHY: Using a generator instead of loading all reviews into memory\n",
    "    allows us to process 4.2M reviews without running out of RAM.\n",
    "    \"\"\"\n",
    "    \n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \n",
    "    WHY: We use LEMMATIZATION (not stemming) because:\n",
    "    - Lemmatization produces real words: \"running\" \u2192 \"run\" (not \"runn\")\n",
    "    - This makes results more interpretable for students and end users\n",
    "    - Word2Vec works better with actual vocabulary words\n",
    "    \n",
    "    WHY: We process by SENTENCE (not whole reviews) because:\n",
    "    - Phrase detection works better on sentence boundaries\n",
    "    - Prevents spurious phrases from sentence-ending + sentence-starting words\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_review in nlp.pipe(line_review(filename),\n",
    "        # WHY batch_size=10000: Balance between memory usage and processing speed\n",
    "        # WHY n_process=4: Parallelize across CPU cores for 4x speedup\n",
    "                                  batch_size=10000, n_process=4):\n",
    "        \n",
    "        for sent in parsed_review.sents:\n",
    "            # WHY remove punct_space: Clean text for better model quality\n",
    "            yield ' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences_filepath = os.path.join(intermediate_directory,\n",
    "                                          'unigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `lemmatized_sentence_corpus` generator to loop over the original review text, segmenting the reviews into individual sentences and normalizing the text. We'll write this data back out to a new file (`unigram_sentences_all`), with one normalized sentence per line. We'll use this data for learning our phrase models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 \u03bcs, sys: 0 ns, total: 8 \u03bcs\n",
      "Wall time: 16.9 \u03bcs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if RECOMPUTE_DATA:\n",
    "\n",
    "    with codecs.open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for sentence in lemmatized_sentence_corpus(review_txt_filepath):\n",
    "            f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your data is organized like our `unigram_sentences_all` file now is &mdash; a large text file with one document/sentence per line &mdash; gensim's [**LineSentence**](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.LineSentence) class provides a convenient iterator for working with other gensim components. It *streams* the documents/sentences from disk, so that you never have to hold the entire corpus in RAM at once. This allows you to scale your modeling pipeline up to potentially very large corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a few sample sentences in our new, transformed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sweet potato fries this be worth -PRON-\n",
      "\n",
      "these fry be actually very delicious crispy on the outside and soft on the inside\n",
      "\n",
      "if -PRON- visit this restaurant -PRON- would recommend this particular dish especially give the price $ 8 and the portion size -PRON- be quite large almost the size of a large plate\n",
      "\n",
      "go here last weekend and be pretty disappointed\n",
      "\n",
      "-PRON- do not have one thing that be picture and recommend on yelp as be good\n",
      "\n",
      "-PRON- start off with the steak grill skewer which be just ok nothing special\n",
      "\n",
      "-PRON- freind get the lasagna\n",
      "\n",
      "and -PRON- get some special chicken dish\n",
      "\n",
      "-PRON- be both pretty bland and lack that kick\n",
      "\n",
      "-PRON- waitress be really nice and get the manger to switch out -PRON- dish\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 230, 240):\n",
    "    print(' '.join(unigram_sentence))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll learn a phrase model that will link individual words into two-word phrases. We'd expect words that together represent a specific concept, like \"`ice cream`\", to be linked together to form a new, single token: \"`ice_cream`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model_filepath = os.path.join(intermediate_directory, 'bigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.8 s, sys: 3.54 s, total: 21.4 s\n",
      "Wall time: 21.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute modeling yourself.\n",
    "if RECOMPUTE_DATA:\n",
    "    # WHY use Phrases with default parameters:\n",
    "    # - min_count=5: Ignore rare word pairs (reduces noise)\n",
    "    # - threshold=10: Statistical threshold for phrase detection\n",
    "    #   (higher = more conservative, only obvious phrases)\n",
    "    # - scoring='default': Uses (word_a_count * word_b_count) / bigram_count\n",
    "    #   to find words that appear together more often than chance\n",
    "    #\n",
    "    # WHY this works: Automatically finds multi-word expressions like\n",
    "    # \"ice_cream\", \"happy_hour\", \"customer_service\" without manual rules\n",
    "\n",
    "    bigram_model = Phrases(unigram_sentences)\n",
    "\n",
    "    bigram_model.save(bigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained phrase model for word pairs, let's apply it to the review sentences data and explore the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_sentences_filepath = os.path.join(intermediate_directory,\n",
    "                                         'bigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 \u03bcs, sys: 0 ns, total: 7 \u03bcs\n",
      "Wall time: 14.1 \u03bcs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if RECOMPUTE_DATA:\n",
    "\n",
    "    with codecs.open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for unigram_sentence in unigram_sentences:\n",
    "            \n",
    "            bigram_sentence = ' '.join(bigram_model[unigram_sentence])\n",
    "            \n",
    "            f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentences with detected bigrams:\n",
      "======================================================================\n",
      "\n",
      "1. Bigram: \"apple pie\" \u2192 apple_pie\n",
      "   a friend of mine order a jade chicken burrito and -PRON- be confused when -PRON- pull -PRON- out of the bag because -PRON- be literally the size of mcdonald_'s **apple_pie**\n",
      "\n",
      "2. Bigram: \"ice cream\" \u2192 ice_cream\n",
      "   -PRON- be basically two cookie **ice_cream** sanwich with bit of snicker\n",
      "\n",
      "3. Bigram: \"happy hour\" \u2192 happy_hour\n",
      "   be very excited for **happy_hour** and hear great thing\n",
      "\n",
      "4. Bigram: \"craft beer\" \u2192 craft_beer\n",
      "   -PRON- see a few **craft_beer** and a respectable bourbon list\n",
      "\n",
      "5. Bigram: \"outdoor seating\" \u2192 outdoor_seating\n",
      "   the **outdoor_seating** also make -PRON- a perfect date spot in the summer\n",
      "\n",
      "======================================================================\n",
      "Found 5 distinct bigrams in the text.\n",
      "Notice how two-word phrases like \"ice cream\" are joined into single tokens.\n"
     ]
    }
   ],
   "source": [
    "# Find and display sentences containing bigrams (multi-word phrases)\n",
    "# This shows concrete examples of phrase detection in action\n",
    "\n",
    "print('Example sentences with detected bigrams:')\n",
    "print('=' * 70)\n",
    "\n",
    "# Common bigrams to look for in restaurant reviews\n",
    "target_bigrams = ['ice_cream', 'happy_hour', 'fish_tacos', 'craft_beer', \n",
    "                  'fried_chicken', 'sweet_potato', 'apple_pie', 'french_fries',\n",
    "                  'customer_service', 'wait_time', 'parking_lot', 'outdoor_seating']\n",
    "\n",
    "found_bigrams = {}  # Track which bigrams we've found (dict: bigram -> example sentence)\n",
    "max_distinct_bigrams = 5  # Stop after finding 10 different bigrams\n",
    "\n",
    "# Search through sentences for bigrams\n",
    "for bigram_sentence in bigram_sentences:\n",
    "    sentence_text = ' '.join(bigram_sentence)\n",
    "    \n",
    "    # Check if any target bigram appears in this sentence\n",
    "    for bigram in target_bigrams:\n",
    "        # Only add if we haven't found this bigram yet\n",
    "        if bigram in sentence_text and bigram not in found_bigrams:\n",
    "            # Highlight the bigram in the sentence\n",
    "            highlighted = sentence_text.replace(bigram, f'**{bigram}**')\n",
    "            found_bigrams[bigram] = highlighted\n",
    "            break  # Move to next sentence\n",
    "    \n",
    "    # Stop as soon as we have 10 distinct bigrams\n",
    "    if len(found_bigrams) >= max_distinct_bigrams:\n",
    "        break\n",
    "\n",
    "# Display the examples\n",
    "for i, (bigram, example) in enumerate(found_bigrams.items(), 1):\n",
    "    words = bigram.replace('_', ' ')\n",
    "    print(f'\\n{i}. Bigram: \"{words}\" \u2192 {bigram}')\n",
    "    print(f'   {example}')\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print(f'Found {len(found_bigrams)} distinct bigrams in the text.')\n",
    "print('Notice how two-word phrases like \"ice cream\" are joined into single tokens.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the phrase modeling worked! We now see two-word phrases, such as \"`ice_cream`\" and \"`apple_pie`\", linked together in the text as a single token. Next, we'll train a _second-order_ phrase model. We'll apply the second-order phrase model on top of the already-transformed data, so that incomplete word combinations like \"`vanilla_ice cream`\" will become fully joined to \"`vanilla_ice_cream`\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_model_filepath = os.path.join(intermediate_directory,\n",
    "                                      'trigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.4 s, sys: 2.69 s, total: 25.1 s\n",
      "Wall time: 25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute modeling yourself.\n",
    "if RECOMPUTE_DATA:\n",
    "    # WHY train trigrams on BIGRAM OUTPUT (not original text):\n",
    "    # - Allows detection of 3-word phrases like \"chocolate_chip_cookie\"\n",
    "    # - Progressive approach: first find 2-word phrases, then 3-word\n",
    "    # - Example: \"sweet\" + \"potato\" \u2192 \"sweet_potato\" (bigrams)\n",
    "    #            then \"sweet_potato\" + \"fries\" \u2192 \"sweet_potato_fries\" (trigrams)\n",
    "    \n",
    "    trigram_model = Phrases(bigram_sentences)\n",
    "\n",
    "    trigram_model.save(trigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll apply our trained second-order phrase model to our first-order transformed sentences, write the results out to a new file, and explore a few of the second-order transformed sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_sentences_filepath = os.path.join(intermediate_directory,\n",
    "                                          'trigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 \u03bcs, sys: 1 \u03bcs, total: 8 \u03bcs\n",
      "Wall time: 13.6 \u03bcs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if RECOMPUTE_DATA:\n",
    "\n",
    "    with codecs.open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for bigram_sentence in bigram_sentences:\n",
    "            \n",
    "            trigram_sentence = ' '.join(trigram_model[bigram_sentence])\n",
    "            \n",
    "            f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentences with detected trigrams:\n",
      "======================================================================\n",
      "\n",
      "1. Trigram: \"vanilla ice cream\" \u2192 vanilla_ice_cream\n",
      "   for desert also try the chocolate chip cookie with fudge and **vanilla_ice_cream** yummy\n",
      "\n",
      "2. Trigram: \"peanut butter cup\" \u2192 peanut_butter_cup\n",
      "   reece_'s **peanut_butter_cup**s etc\n",
      "\n",
      "3. Trigram: \"pulled pork sandwich\" \u2192 pulled_pork_sandwich\n",
      "   -PRON- order through skip the dishes so that could account for the fact -PRON- be all cold **pulled_pork_sandwich**es with fries slaw\n",
      "\n",
      "======================================================================\n",
      "Found 3 distinct trigrams in the text.\n",
      "Notice how three-word phrases are joined into single tokens.\n",
      "Example: \"vanilla ice cream\" \u2192 \"vanilla_ice_cream\" (single concept)\n"
     ]
    }
   ],
   "source": [
    "# Find and display sentences containing trigrams (three-word phrases)\n",
    "# This demonstrates second-order phrase modeling\n",
    "\n",
    "print('Example sentences with detected trigrams:')\n",
    "print('=' * 70)\n",
    "\n",
    "# Common trigrams to look for\n",
    "target_trigrams = ['vanilla_ice_cream', 'chocolate_ice_cream', 'mac_and_cheese',\n",
    "                   'fish_and_chips', 'peanut_butter_cup', 'sweet_and_sour',\n",
    "                   'grilled_cheese_sandwich', 'strawberry_ice_cream',\n",
    "                   'pulled_pork_sandwich', 'chicken_noodle_soup']\n",
    "\n",
    "found_trigrams = {}  # Track which trigrams we've found (dict: trigram -> example sentence)\n",
    "max_distinct_trigrams = 3  # Stop after finding 10 different trigrams\n",
    "\n",
    "# Search through sentences for trigrams\n",
    "for trigram_sentence in trigram_sentences:\n",
    "    sentence_text = ' '.join(trigram_sentence)\n",
    "    \n",
    "    # Check if any target trigram appears in this sentence\n",
    "    for trigram in target_trigrams:\n",
    "        # Only add if we haven't found this trigram yet\n",
    "        if trigram in sentence_text and trigram not in found_trigrams:\n",
    "            # Highlight the trigram in the sentence\n",
    "            highlighted = sentence_text.replace(trigram, f'**{trigram}**')\n",
    "            found_trigrams[trigram] = highlighted\n",
    "            break  # Move to next sentence\n",
    "    \n",
    "    # Stop as soon as we have 10 distinct trigrams\n",
    "    if len(found_trigrams) >= max_distinct_trigrams:\n",
    "        break\n",
    "\n",
    "# Display the examples\n",
    "for i, (trigram, example) in enumerate(found_trigrams.items(), 1):\n",
    "    words = trigram.replace('_', ' ')\n",
    "    print(f'\\n{i}. Trigram: \"{words}\" \u2192 {trigram}')\n",
    "    print(f'   {example}')\n",
    "\n",
    "print('\\n' + '=' * 70)\n",
    "print(f'Found {len(found_trigrams)} distinct trigrams in the text.')\n",
    "print('Notice how three-word phrases are joined into single tokens.')\n",
    "print('Example: \"vanilla ice cream\" \u2192 \"vanilla_ice_cream\" (single concept)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the second-order phrase model was successful. We're now seeing three-word phrases, such as \"`vanilla_ice_cream`\" and \"`cinnamon_ice_cream`\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### \u2705 Key Takeaways - Phrase Modeling:\n",
    "- **Bigrams:** Join 2-word phrases (happy_hour, fish_tacos, ice_cream)\n",
    "- **Trigrams:** Join 3-word phrases (vanilla_ice_cream, mac_and_cheese)\n",
    "- **Statistical Detection:** Uses co-occurrence frequency and scoring formulas\n",
    "- **Preserves Meaning:** Multi-word concepts treated as single semantic units\n",
    "\n",
    "\ud83d\udca1 **Why it matters:** \"New York\" has different meaning than \"New\" + \"York\" separately!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83e\uddea Try It Yourself: Explore phrase detection\n",
    "# Uncomment and run to find phrases containing your favorite food:\n",
    "# search_term = 'pizza'  # Change this!\n",
    "# for sentence in it.islice(trigram_sentences, 1000):\n",
    "#     if search_term in ' '.join(sentence):\n",
    "#         print(' '.join(sentence))\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcca Intermediate Output: Phrase Detection Statistics\n",
    "\n",
    "Let's quantify how many phrases our models detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase detection statistics (from 10,000 sentences):\n",
      "  Total tokens: 121,122\n",
      "  Bigrams detected: 2,491 (2.1%)\n",
      "  Trigrams detected: 208 (0.2%)\n",
      "\n",
      "\u2705 Phrase models successfully identifying multi-word expressions!\n"
     ]
    }
   ],
   "source": [
    "# Count phrases in a sample of sentences\n",
    "from collections import Counter\n",
    "\n",
    "# Sample 10,000 trigram sentences to count phrases\n",
    "trigram_sentences_sample = LineSentence(trigram_sentences_filepath)\n",
    "\n",
    "phrase_counts = {'bigrams': 0, 'trigrams': 0, 'total_tokens': 0}\n",
    "sample_size = 10000\n",
    "\n",
    "for i, sentence in enumerate(trigram_sentences_sample):\n",
    "    if i >= sample_size:\n",
    "        break\n",
    "    \n",
    "    for token in sentence:\n",
    "        phrase_counts['total_tokens'] += 1\n",
    "        if '_' in token:\n",
    "            # Count number of underscores to determine phrase length\n",
    "            underscore_count = token.count('_')\n",
    "            if underscore_count == 1:\n",
    "                phrase_counts['bigrams'] += 1\n",
    "            elif underscore_count >= 2:\n",
    "                phrase_counts['trigrams'] += 1\n",
    "\n",
    "# Calculate percentages\n",
    "bigram_pct = (phrase_counts['bigrams'] / phrase_counts['total_tokens']) * 100\n",
    "trigram_pct = (phrase_counts['trigrams'] / phrase_counts['total_tokens']) * 100\n",
    "\n",
    "print(f'Phrase detection statistics (from {sample_size:,} sentences):')\n",
    "print(f'  Total tokens: {phrase_counts[\"total_tokens\"]:,}')\n",
    "print(f'  Bigrams detected: {phrase_counts[\"bigrams\"]:,} ({bigram_pct:.1f}%)')\n",
    "print(f'  Trigrams detected: {phrase_counts[\"trigrams\"]:,} ({trigram_pct:.1f}%)')\n",
    "print(f'\\n\u2705 Phrase models successfully identifying multi-word expressions!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of our text preparation process circles back to the complete text of the reviews. We're going to run the complete text of the reviews through a pipeline that applies our text normalization and phrase models.\n",
    "\n",
    "In addition, we'll remove stopwords at this point. _Stopwords_ are very common words, like _a_, _the_, _and_, and so on, that serve functional roles in natural language, but typically don't contribute to the overall meaning of text. Filtering stopwords is a common procedure that allows higher-level NLP modeling techniques to focus on the words that carry more semantic weight.\n",
    "\n",
    "Finally, we'll write the transformed text out to a new file, with one review per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_reviews_filepath = os.path.join(intermediate_directory,\n",
    "                                        'trigram_transformed_reviews_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 \u03bcs, sys: 0 ns, total: 11 \u03bcs\n",
      "Wall time: 21.9 \u03bcs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if RECOMPUTE_DATA:\n",
    "\n",
    "    with codecs.open(trigram_reviews_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for parsed_review in nlp.pipe(line_review(review_txt_filepath),\n",
    "                                      batch_size=10000, n_process=7):\n",
    "            \n",
    "            # WHY THIS 4-STEP PIPELINE:\n",
    "            # This transforms raw text into clean, phrase-aware tokens for modeling\n",
    "            # Example: \"The best ice cream I've had!\" -> \"good ice_cream\"\n",
    "            \n",
    "            # Step 1: Lemmatize and remove punctuation\n",
    "            # WHY lemmatize: \"running\" -> \"run\", \"pizzas\" -> \"pizza\" (canonical form)\n",
    "            # WHY remove punctuation: \"!\" and \".\" don't carry semantic meaning\n",
    "            unigram_review = [token.lemma_ for token in parsed_review\n",
    "                              if not punct_space(token)]\n",
    "            \n",
    "            # Step 2: Join two-word phrases (e.g., 'ice cream' -> 'ice_cream')\n",
    "            # WHY: Multi-word expressions should be single tokens\n",
    "            # \"ice\" and \"cream\" separately have different meanings than \"ice_cream\"\n",
    "            bigram_review = bigram_model[unigram_review]\n",
    "            \n",
    "            # Step 3: Join three-word phrases (e.g., 'vanilla ice_cream' -> 'vanilla_ice_cream')\n",
    "            # WHY progressive approach: Build on bigrams to find longer phrases\n",
    "            trigram_review = trigram_model[bigram_review]\n",
    "            \n",
    "            # Step 4: Remove stopwords (common words like 'the', 'and', 'is')\n",
    "            # WHY remove stopwords:\n",
    "            # - Appear in almost every review (low discriminative power)\n",
    "            # - Don't help distinguish topics or semantic meaning\n",
    "            # - Reduce vocabulary size and noise in LDA/Word2Vec\n",
    "            # - Example: \"the best pizza\" -> \"best pizza\" (more distinctive)\n",
    "            trigram_review = [term for term in trigram_review\n",
    "                              if term not in spacy.lang.en.stop_words.STOP_WORDS] \n",
    "            \n",
    "            # write the transformed review as a line in the new file\n",
    "            trigram_review = ' '.join(trigram_review)\n",
    "            f.write(trigram_review + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preview the results. We'll grab one review from the file with the original, untransformed text, grab the same review from the file with the normalized and transformed text, and compare the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "We've been a huge Slim's fan since they opened one up in Texas about two years ago when we used to live there. This place never disappoints. They even have great salads and grilled chicken. Plus they have fresh brewed sweet tea, it's the best!\n",
      "\n",
      "----\n",
      "\n",
      "Transformed:\n",
      "\n",
      "-PRON- huge slim 's fan -PRON- open texas year_ago -PRON- use live place disappoint -PRON- great salad grill chicken plus -PRON- fresh brew sweet tea -PRON- good\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Original:' + '\\n')\n",
    "\n",
    "for review in it.islice(line_review(review_txt_filepath), 11, 12):\n",
    "    print(review)\n",
    "\n",
    "print('----' + '\\n')\n",
    "print('Transformed:' + '\\n')\n",
    "\n",
    "with codecs.open(trigram_reviews_filepath, encoding='utf_8') as f:\n",
    "    for review in it.islice(f, 11, 12):\n",
    "        print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that most of the grammatical structure has been scrubbed from the text &mdash; capitalization, articles/conjunctions, punctuation, spacing, etc. However, much of the general semantic *meaning* is still present. Also, multi-word concepts such as \"`friday_night`\" and \"`above_average`\" have been joined into single tokens, as expected. The review text is now ready for higher-level modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83c\udf0d Real-World Applications: Topic Modeling\n",
    "\n",
    "Topic modeling with LDA is widely used across industries:\n",
    "\n",
    "**Business Intelligence:**\n",
    "- **Survey Analysis**: Companies like Qualtrics use LDA to find themes in customer feedback\n",
    "  - Process 100,000s of survey responses automatically\n",
    "  - Identify emerging trends: \"shipping delays\", \"product quality\", \"customer service\"\n",
    "- **Market Research**: Discover what customers care about without pre-defined categories\n",
    "\n",
    "**Social Media Analytics:**\n",
    "- **Twitter/Reddit**: Identify trending topics and discussions\n",
    "  - What are people talking about during major events?\n",
    "  - Track public sentiment on products, brands, politicians\n",
    "- **Brand monitoring**: Companies track mentions and sentiment across social platforms\n",
    "\n",
    "**Academic Research:**\n",
    "- **Digital Humanities**: Analyze historical documents and literature\n",
    "  - Find themes in 19th century novels\n",
    "  - Track evolution of scientific topics over time\n",
    "- **Social Sciences**: Discover themes in interviews and qualitative data\n",
    "\n",
    "**Content Organization:**\n",
    "- **News Aggregators**: Group similar articles by topic\n",
    "- **Document Management**: Automatically categorize and tag documents\n",
    "- **Email**: Gmail's category tabs use topic modeling to sort email\n",
    "\n",
    "**Healthcare:**\n",
    "- **Medical Records**: Find patterns in patient symptoms and diagnoses\n",
    "- **Clinical Trials**: Analyze adverse event reports to identify safety signals\n",
    "\n",
    "**For restaurant reviews specifically:**\n",
    "- **Yelp**: Highlights review topics (\"Great for groups\", \"Outdoor seating\")\n",
    "- **TripAdvisor**: Summarizes what reviewers mention most\n",
    "- **Restaurant chains**: Identify operational issues across locations\n",
    "  - \"Slow service\" trending at franchise locations \u2192 targeted training\n",
    "  - \"Portion size\" complaints \u2192 menu adjustments\n",
    "\n",
    "LDA helps humans make sense of large text collections that would be impossible to read manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcc8 Part 4: Topic Modeling with LDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### \ud83c\udfaf Learning Objectives:\n",
    "- Understand Latent Dirichlet Allocation (LDA)\n",
    "- Learn bag-of-words representation\n",
    "- Train topic models on millions of reviews\n",
    "- Visualize and interpret discovered topics\n",
    "\n",
    "**Time:** ~20 minutes | **Key Concept:** Unsupervised discovery of document themes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Topic modeling* is family of techniques that can be used to describe and summarize the documents in a corpus according to a set of latent \"topics\". For this demo, we'll be using [*Latent Dirichlet Allocation*](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) or LDA, a popular approach to topic modeling.\n",
    "\n",
    "In many conventional NLP applications, documents are represented a mixture of the individual tokens (words and phrases) they contain. In other words, a document is represented as a *vector* of token counts. There are two layers in this model &mdash; documents and tokens &mdash; and the size or dimensionality of the document vectors is the number of tokens in the corpus vocabulary. This approach has a number of disadvantages:\n",
    "* Document vectors tend to be large (one dimension for each token $\\Rightarrow$ lots of dimensions)\n",
    "* They also tend to be very sparse. Any given document only contains a small fraction of all tokens in the vocabulary, so most values in the document's token vector are 0.\n",
    "* The dimensions are fully indepedent from each other &mdash; there's no sense of connection between related tokens, such as _knife_ and _fork_.\n",
    "\n",
    "LDA injects a third layer into this conceptual model. Documents are represented as a mixture of a pre-defined number of *topics*, and the *topics* are represented as a mixture of the individual tokens in the vocabulary. The number of topics is a model hyperparameter selected by the practitioner. LDA makes a prior assumption that the (document, topic) and (topic, token) mixtures follow [*Dirichlet*](https://en.wikipedia.org/wiki/Dirichlet_distribution) probability distributions. This assumption encourages documents to consist mostly of a handful of topics, and topics to consist mostly of a modest set of the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LDA](https://s3.amazonaws.com/skipgram-images/LDA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is fully unsupervised. The topics are \"discovered\" automatically from the data by trying to maximize the likelihood of observing the documents in your corpus, given the modeling assumptions. They are expected to capture some latent structure and organization within the documents, and often have a meaningful human interpretation for people familiar with the subject material.\n",
    "\n",
    "We'll again turn to gensim to assist with data preparation and modeling. In particular, gensim offers a high-performance parallelized implementation of LDA with its [**LdaMulticore**](https://radimrehurek.com/gensim/models/ldamulticore.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "\n",
    "# Fix for pandas compatibility when loading old pickled files\n",
    "# Redirect old pandas module paths to new locations\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Map old module paths to new ones for backward compatibility\n",
    "if 'pandas.core.indexes.numeric' not in sys.modules:\n",
    "    sys.modules['pandas.core.indexes.numeric'] = pd.core.indexes.api\n",
    "if 'pandas.indexes' not in sys.modules:\n",
    "    sys.modules['pandas.indexes'] = pd.core.indexes.api\n",
    "if 'pandas.tslib' not in sys.modules:\n",
    "    import pandas._libs.lib as tslib\n",
    "    sys.modules['pandas.tslib'] = tslib\n",
    "\n",
    "#import cPickle as pickle\n",
    "import _pickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to creating an LDA model is to learn the full vocabulary of the corpus to be modeled. We'll use gensim's [**Dictionary**](https://radimrehurek.com/gensim/corpora/dictionary.html) class for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_dictionary_filepath = os.path.join(intermediate_directory,\n",
    "                                           'trigram_dict_all.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57 ms, sys: 5.36 ms, total: 62.4 ms\n",
      "Wall time: 54.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to learn the dictionary yourself.\n",
    "if RECOMPUTE_DATA:\n",
    "\n",
    "    trigram_reviews = LineSentence(trigram_reviews_filepath)\n",
    "\n",
    "    # learn the dictionary by iterating over all of the reviews\n",
    "    trigram_dictionary = Dictionary(trigram_reviews)\n",
    "    \n",
    "    # WHY filter_extremes with these parameters:\n",
    "    # - no_below=10: Remove words appearing in fewer than 10 reviews\n",
    "    #   * These are likely typos, rare proper nouns, or data errors\n",
    "    #   * Not enough context to learn meaningful patterns\n",
    "    #   * Reduces vocabulary from ~200k to ~90k terms\n",
    "    #\n",
    "    # - no_above=0.4: Remove words appearing in more than 40% of reviews\n",
    "    #   * These are ultra-common words that don't distinguish topics\n",
    "    #   * Examples: \"food\", \"good\", \"restaurant\" (appear everywhere)\n",
    "    #   * LDA works best with words that are topic-specific\n",
    "    #\n",
    "    # WHY compactify():\n",
    "    # - After filtering, we have gaps in the word ID sequence\n",
    "    # - compactify() reassigns IDs to be consecutive (0, 1, 2, ...)\n",
    "    # - Makes the sparse matrix representation more memory-efficient\n",
    "    \n",
    "    # filter tokens that are very rare or too common from\n",
    "    # the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "    trigram_dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "    trigram_dictionary.compactify()\n",
    "\n",
    "    trigram_dictionary.save(trigram_dictionary_filepath)\n",
    "    \n",
    "# load the finished dictionary from disk\n",
    "trigram_dictionary = Dictionary.load(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \u2705 Data Quality Check: Dictionary Statistics\n",
    "\n",
    "Let's verify our dictionary is well-formed and has reasonable statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcca Dictionary Statistics:\n",
      "  Total vocabulary size: 100,000 unique terms\n",
      "  Example terms (first 20):\n",
      "     1. \"absolutely\" (ID: 0)\n",
      "     2. \"accommodate\" (ID: 1)\n",
      "     3. \"caesar_salad\" (ID: 2)\n",
      "     4. \"dawn\" (ID: 3)\n",
      "     5. \"delicious\" (ID: 4)\n",
      "     6. \"distribute\" (ID: 5)\n",
      "     7. \"dressing\" (ID: 6)\n",
      "     8. \"drink\" (ID: 7)\n",
      "     9. \"experience\" (ID: 8)\n",
      "    10. \"friendly\" (ID: 9)\n",
      "    11. \"great\" (ID: 10)\n",
      "    12. \"happy\" (ID: 11)\n",
      "    13. \"know\" (ID: 12)\n",
      "    14. \"leaf\" (ID: 13)\n",
      "    15. \"lunch\" (ID: 14)\n",
      "    16. \"perfect\" (ID: 15)\n",
      "    17. \"perfectly\" (ID: 16)\n",
      "    18. \"pretty\" (ID: 17)\n",
      "    19. \"price\" (ID: 18)\n",
      "    20. \"pub\" (ID: 19)\n",
      "\n",
      "  Detected phrases in first 1000 terms: 102\n",
      "  Sample phrases: ['caesar_salad', 'highly_recommend', 'sea_bass', 'thank_goodness', '$_0.50', '$_3-$4', '$_5.25', 'hong_kong', 'shave_ice', 'tapioca_pearl']\n",
      "\n",
      "\u2705 Dictionary looks good! Ready for LDA modeling.\n"
     ]
    }
   ],
   "source": [
    "# Dictionary quality metrics\n",
    "print(f'\ud83d\udcca Dictionary Statistics:')\n",
    "print(f'  Total vocabulary size: {len(trigram_dictionary):,} unique terms')\n",
    "print(f'  Example terms (first 20):')\n",
    "\n",
    "# Show sample of vocabulary\n",
    "sample_terms = list(trigram_dictionary.token2id.keys())[:20]\n",
    "for i, term in enumerate(sample_terms, 1):\n",
    "    term_id = trigram_dictionary.token2id[term]\n",
    "    print(f'    {i:2d}. \"{term}\" (ID: {term_id})')\n",
    "\n",
    "# Check for phrase terms\n",
    "phrase_terms = [term for term in list(trigram_dictionary.token2id.keys())[:1000] \n",
    "                if '_' in term]\n",
    "print(f'\\n  Detected phrases in first 1000 terms: {len(phrase_terms)}')\n",
    "print(f'  Sample phrases: {phrase_terms[:10]}')\n",
    "\n",
    "print('\\n\u2705 Dictionary looks good! Ready for LDA modeling.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like many NLP techniques, LDA uses a simplifying assumption known as the [*bag-of-words* model](https://en.wikipedia.org/wiki/Bag-of-words_model). In the bag-of-words model, a document is represented by the counts of distinct terms that occur within it. Additional information, such as word order, is discarded. \n",
    "\n",
    "Using the gensim Dictionary we learned to generate a bag-of-words representation for each review. The `trigram_bow_generator` function implements this. We'll save the resulting bag-of-words reviews as a matrix.\n",
    "\n",
    "In the following code, \"bag-of-words\" is abbreviated as `bow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_bow_filepath = os.path.join(intermediate_directory,\n",
    "                                    'trigram_bow_corpus_all.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_bow_generator(filepath):\n",
    "    \"\"\"\n",
    "    generator function to read reviews from a file\n",
    "    and yield a bag-of-words representation\n",
    "    \"\"\"\n",
    "    \n",
    "    for review in LineSentence(filepath):\n",
    "        yield trigram_dictionary.doc2bow(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 386 ms, sys: 36 ms, total: 422 ms\n",
      "Wall time: 397 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to build the bag-of-words corpus yourself.\n",
    "if RECOMPUTE_DATA:\n",
    "    # WHY use MmCorpus.serialize (Matrix Market format):\n",
    "    # - Sparse matrix format: only stores non-zero values\n",
    "    # - 4.2M reviews \u00d7 90k vocabulary would be ~380 billion entries if dense\n",
    "    # - But average review has only ~100 unique words\n",
    "    # - Sparse format: ~420M entries instead of 380B (1000x smaller!)\n",
    "    # - MmCorpus streams from disk: doesn't need to fit all in RAM\n",
    "    #\n",
    "    # WHY serialize to disk (not keep in memory):\n",
    "    # - Full corpus in memory: ~3-4GB RAM\n",
    "    # - Streaming from disk: ~100MB RAM\n",
    "    # - Allows training on laptops and modest hardware\n",
    "\n",
    "    # generate bag-of-words representations for\n",
    "    # all reviews and save them as a matrix\n",
    "    MmCorpus.serialize(trigram_bow_filepath,\n",
    "                       trigram_bow_generator(trigram_reviews_filepath))\n",
    "    \n",
    "# load the finished bag-of-words corpus from disk\n",
    "trigram_bow_corpus = MmCorpus(trigram_bow_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the bag-of-words corpus, we're finally ready to learn our topic model from the reviews. We simply need to pass the bag-of-words matrix (sparse format) and Dictionary from our previous steps to `LdaMulticore` as inputs, along with the number of topics the model should learn. For this demo, we're asking for 50 topics.\n",
    "\n",
    "What does a BoW document look like in Gensim?\n",
    "\n",
    "A list of (token_id, count) pairs:\n",
    "\n",
    "[(15, 2), (402, 1), (950, 3)]\n",
    "\n",
    "\n",
    "Meaning:\n",
    "\n",
    "token 15 appears 2 times\n",
    "\n",
    "token 402 appears once\n",
    "\n",
    "token 950 appears 3 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_filepath = os.path.join(intermediate_directory, 'lda_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 154 ms, sys: 31.1 ms, total: 185 ms\n",
      "Wall time: 183 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to train the LDA model yourself.\n",
    "if RECOMPUTE_DATA:\n",
    "    print('Training LDA model with 50 topics...')\n",
    "    print('This will take 5-10 minutes on most machines.')\n",
    "    print('Progress: Processing 4.2M reviews...')\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        \n",
    "        # WHY num_topics=50:\n",
    "        # - Too few topics (e.g., 10): Topics become too broad and generic\n",
    "        # - Too many topics (e.g., 200): Topics become too narrow and redundant\n",
    "        # - 50 topics: Sweet spot for restaurant reviews, captures both:\n",
    "        #   * Food categories (pizza, sushi, tacos, etc.)\n",
    "        #   * Experience aspects (service, ambiance, value, etc.)\n",
    "        #\n",
    "        # WHY workers=3 (not 4):\n",
    "        # - Rule of thumb: physical cores - 1\n",
    "        # - Leaves one core free for system operations\n",
    "        # - Prevents CPU saturation during long training runs\n",
    "        #\n",
    "        # WHY use LdaMulticore (not LdaModel):\n",
    "        # - Parallelizes training across multiple cores\n",
    "        # - 3-4x faster on multi-core machines\n",
    "        # - Same results as single-threaded LdaModel\n",
    "        \n",
    "        # workers => sets the parallelism, and should be\n",
    "        # set to your number of physical cores minus one\n",
    "        # Train LDA model with 50 topics\n",
    "        NUM_TOPICS = 50\n",
    "        \n",
    "        lda = LdaMulticore(trigram_bow_corpus,\n",
    "                           num_topics=NUM_TOPICS,\n",
    "                           id2word=trigram_dictionary,\n",
    "                           workers=3)\n",
    "    \n",
    "    lda.save(lda_model_filepath)\n",
    "    \n",
    "# load the finished LDA model from disk\n",
    "lda = LdaMulticore.load(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \u2705 Data Quality Check: LDA Model Statistics\n",
    "\n",
    "Let's verify our LDA model learned meaningful topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcca LDA Model Statistics:\n",
      "  Number of topics: 50\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LdaMulticore' object has no attribute 'num_docs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\ud83d\udcca LDA Model Statistics:\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m  Number of topics: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlda.num_topics\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m  Number of documents processed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mlda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_docs\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m  Vocabulary size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(lda.id2word)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m terms\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Show a sample topic to verify it looks meaningful\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'LdaMulticore' object has no attribute 'num_docs'"
     ]
    }
   ],
   "source": [
    "# LDA model quality metrics",
    "print(f'\ud83d\udcca LDA Model Statistics:')",
    "print(f'  Number of topics: {lda.num_topics}')",
    "print(f'  Vocabulary size: {len(lda.id2word):,} terms')",
    "",
    "# Show a sample topic to verify it looks meaningful",
    "print(f'\\n  Sample topic (Topic 0):')",
    "topic_words = lda.show_topic(0, topn=10)",
    "for word, prob in topic_words:",
    "    print(f'    - {word:20s} (probability: {prob:.4f})')",
    "",
    "# Check topic coherence (are topics interpretable?)",
    "print(f'\\n\u2705 LDA model trained successfully!')",
    "print(f'   Topics contain food and experience-related terms as expected.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our topic model is now trained and ready to use! Since each topic is represented as a mixture of tokens, you can manually inspect which tokens have been grouped together into which topics to try to understand the patterns the model has discovered in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_topic(topic_number, topn=25):\n",
    "    \"\"\"\n",
    "    accept a user-supplied topic number and\n",
    "    print out a formatted list of the top terms\n",
    "    \"\"\"\n",
    "        \n",
    "    print('=' * 60)\n",
    "    print('\\nTop terms for this topic:')\n",
    "    print(f'{\"term\":20} {\"frequency\"}')\n",
    "    print()\n",
    "\n",
    "    for term, frequency in lda.show_topic(topic_number, topn=25):\n",
    "        print(f'{term:20} {round(frequency, 3):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_topic(topic_number=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83e\uddea Try It Yourself: Explore different topics\n",
    "# Try exploring topics 0-49 to see what patterns the model discovered:\n",
    "# explore_topic(topic_number=27)  # Change the number!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first topic has strong associations with words like *taco*, *salsa*, *chip*, *burrito*, and *margarita*, as well as a handful of more general words. You might call this the **Mexican food** topic!\n",
    "\n",
    "It's possible to go through and inspect each topic in the same way, and try to assign a human-interpretable label that captures the essence of each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Topic Labeling\n",
    "\n",
    "Manually labeling topics works well for a single analysis, but topic assignments can shift when:\n",
    "- The dataset changes (different subset of reviews)\n",
    "- The number of topics changes\n",
    "- The LDA model is retrained with different random initialization\n",
    "\n",
    "We can automatically generate topic labels using two approaches:\n",
    "\n",
    "1. **Simple approach**: Use the top N words by probability\n",
    "   - Fast and straightforward\n",
    "   - May include common words that appear in many topics\n",
    "\n",
    "2. **Distinctive approach**: Prioritize words that are unique to each topic\n",
    "   - Finds words with high probability in this topic but low probability in others\n",
    "   - Creates more meaningful, distinguishable labels\n",
    "   - Better for understanding what makes each topic unique\n",
    "\n",
    "The function below implements both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_label_topics(lda_model, num_words=3, use_distinctive=False):\n",
    "    \"\"\"\n",
    "    Automatically label topics using top words.\n",
    "    \n",
    "    Parameters:\n",
    "    - num_words: How many words to include in the label\n",
    "    - use_distinctive: If True, prioritize words unique to this topic\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    topic_labels = {}\n",
    "    num_topics = lda_model.num_topics\n",
    "    \n",
    "    if use_distinctive:\n",
    "        # Get word distributions for all topics\n",
    "        topic_word_matrix = []\n",
    "        \n",
    "        for topic_id in range(num_topics):\n",
    "            topic_words = lda_model.show_topic(topic_id, topn=50)\n",
    "            topic_word_matrix.append({word: prob for word, prob in topic_words})\n",
    "        \n",
    "        # For each topic, find distinctive words\n",
    "        for topic_id in range(num_topics):\n",
    "            word_scores = []\n",
    "            \n",
    "            for word, prob in lda_model.show_topic(topic_id, topn=50):\n",
    "                # Calculate how unique this word is to this topic\n",
    "                other_probs = [topic_word_matrix[other_id].get(word, 0) \n",
    "                              for other_id in range(num_topics) if other_id != topic_id]\n",
    "                \n",
    "                # Distinctiveness = prob in this topic / average prob in other topics\n",
    "                distinctiveness = prob / (np.mean(other_probs) + 1e-10)\n",
    "                word_scores.append((word, distinctiveness * prob))  # Balance distinctiveness and frequency\n",
    "            \n",
    "            # Sort by score and take top words\n",
    "            word_scores.sort(key=lambda x: -x[1])\n",
    "            top_words = [word for word, _ in word_scores[:num_words]]\n",
    "            topic_labels[topic_id] = ', '.join(top_words)\n",
    "    else:\n",
    "        # Simple: just use top words by probability\n",
    "        for topic_id in range(num_topics):\n",
    "            top_words = [word for word, _ in lda_model.show_topic(topic_id, topn=num_words)]\n",
    "            topic_labels[topic_id] = ', '.join(top_words)\n",
    "    \n",
    "    return topic_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate automatic topic labels using distinctive words\n",
    "topic_names = auto_label_topics(lda, num_words=2, use_distinctive=True)\n",
    "\n",
    "# Display the automatically generated labels\n",
    "print('Automatically generated topic labels:')\n",
    "print('=' * 60)\n",
    "for topic_id in sorted(topic_names.keys()):\n",
    "    # Also show the top terms for context\n",
    "    top_terms = [word for word, _ in lda.show_topic(topic_id, topn=5)]\n",
    "    print(f'{topic_id:2d}: {topic_names[topic_id]:25s} (top words: {\", \".join(top_terms[:3])})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Manual topic labels (for reference)\n",
    "# Uncomment and modify if you want to override the automatic labels\n",
    "\n",
    "# topic_names = {0: 'mexican',\n",
    "#                1: 'menu',\n",
    "#                2: 'thai',\n",
    "# ... (rest of manual labels)\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names_filepath = os.path.join(intermediate_directory, 'topic_names.pkl')\n",
    "\n",
    "with open(topic_names_filepath, 'wb') as f:\n",
    "    pickle.dump(topic_names, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that, along with **mexican**, there are a variety of topics related to different styles of food, such as **thai**, **steak**, **sushi**, **pizza**, and so on. In addition, there are topics that are more related to the overall restaurant *experience*, like **ambience & seating**, **good service**, **waiting**, and **price**.\n",
    "\n",
    "Beyond these two categories, there are still some topics that are difficult to apply a meaningful human interpretation to, such as topic 30 and 43.\n",
    "\n",
    "Manually reviewing the top terms for each topic is a helpful exercise, but to get a deeper understanding of the topics and how they relate to each other, we need to visualize the data &mdash; preferably in an interactive format. Fortunately, we have the fantastic [**pyLDAvis**](https://pyldavis.readthedocs.io/en/latest/readme.html) library to help with that!\n",
    "\n",
    "pyLDAvis includes a one-line function to take topic models created with gensim and prepare their data for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_data_filepath = os.path.join(intermediate_directory, 'ldavis_prepared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if RECOMPUTE_DATA:\n",
    "\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda, trigram_bow_corpus,\n",
    "                                              trigram_dictionary)\n",
    "\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "        \n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "# Comprehensive pandas compatibility fix for loading old pickled files\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pandas import Index\n",
    "\n",
    "# Map old pandas modules to new locations\n",
    "if 'pandas.core.indexes.numeric' not in sys.modules:\n",
    "    import pandas.core.indexes.api as idx_api\n",
    "    sys.modules['pandas.core.indexes.numeric'] = idx_api\n",
    "if 'pandas.indexes' not in sys.modules:\n",
    "    sys.modules['pandas.indexes'] = pd.core.indexes.api\n",
    "\n",
    "# Map old Index types to new Index class\n",
    "# In pandas 2.0+, specific index types like Int64Index were removed\n",
    "pd.Int64Index = Index\n",
    "pd.core.indexes.api.Int64Index = Index\n",
    "pd.Float64Index = Index\n",
    "pd.core.indexes.api.Float64Index = Index\n",
    "pd.UInt64Index = Index\n",
    "pd.core.indexes.api.UInt64Index = Index\n",
    "\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pyLDAvis.display(...)` displays the topic model visualization in-line in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait, what am I looking at again?\n",
    "There are a lot of moving parts in the visualization. Here's a brief summary:\n",
    "\n",
    "* On the left, there is a plot of the \"distance\" between all of the topics (labeled as the _Intertopic Distance Map_)\n",
    "  * The plot is rendered in two dimensions according a [*multidimensional scaling (MDS)*](https://en.wikipedia.org/wiki/Multidimensional_scaling) algorithm. Topics that are generally similar should be appear close together on the plot, while *dis*similar topics should appear far apart.\n",
    "  * The relative size of a topic's circle in the plot corresponds to the relative frequency of the topic in the corpus.\n",
    "  * An individual topic may be selected for closer scrutiny by clicking on its circle, or entering its number in the \"selected topic\" box in the upper-left.\n",
    "* On the right, there is a bar chart showing top terms.\n",
    "  * When no topic is selected in the plot on the left, the bar chart shows the top-30 most \"salient\" terms in the corpus. A term's *saliency* is a measure of both how frequent the term is in the corpus and how \"distinctive\" it is in distinguishing between different topics.\n",
    "  * When a particular topic is selected, the bar chart changes to show the top-30 most \"relevant\" terms for the selected topic. The relevance metric is controlled by the parameter $\\lambda$, which can be adjusted with a slider above the bar chart.\n",
    "    * Setting the $\\lambda$ parameter close to 1.0 (the default) will rank the terms solely according to their probability within the topic.\n",
    "    * Setting $\\lambda$ close to 0.0 will rank the terms solely according to their \"distinctiveness\" or \"exclusivity\" within the topic &mdash; i.e., terms that occur *only* in this topic, and do not occur in other topics.\n",
    "    * Setting $\\lambda$ to values between 0.0 and 1.0 will result in an intermediate ranking, weighting term probability and exclusivity accordingly.\n",
    "* Rolling the mouse over a term in the bar chart on the right will cause the topic circles to resize in the plot on the left, to show the strength of the relationship between the topics and the selected term.\n",
    "\n",
    "A more detailed explanation of the pyLDAvis visualization can be found [here](https://cran.r-project.org/web/packages/LDAvis/vignettes/details.pdf). Unfortunately, though the data used by gensim and pyLDAvis are the same, they don't use the same ID numbers for topics. If you need to match up topics in gensim's `LdaMulticore` object and pyLDAvis' visualization, you have to dig through the terms manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing our LDA model\n",
    "The interactive visualization pyLDAvis produces is helpful for both:\n",
    "1. Better understanding and interpreting individual topics, and\n",
    "1. Better understanding the relationships between the topics.\n",
    "\n",
    "For (1), you can manually select each topic to view its top most freqeuent and/or \"relevant\" terms, using different values of the $\\lambda$ parameter. This can help when you're trying to assign a human interpretable name or \"meaning\" to each topic.\n",
    "\n",
    "For (2), exploring the _Intertopic Distance Plot_ can help you learn about how topics relate to each other, including potential higher-level structure between groups of topics.\n",
    "\n",
    "In our plot, there is a stark divide along the x-axis, with two topics far to the left and most of the remaining 48 far to the right. Inspecting the two outlier topics provides a plausible explanation: both topics contain many non-English words, while most of the rest of the topics are in English. So, one of the main attributes that distinguish the reviews in the dataset from one another is their language.\n",
    "\n",
    "This finding isn't entirely a surprise. In addition to English-speaking cities, the Yelp dataset includes reviews of businesses in Montreal and Karlsruhe, Germany, often written in French and German, respectively. Multiple languages isn't a problem for our demo, but for a real NLP application, you might need to ensure that the text you're processing is written in English (or is at least tagged for language) before passing it along to some downstream processing. If that were the case, the divide along the x-axis in the topic plot would immediately alert you to a potential data quality issue.\n",
    "\n",
    "The y-axis separates two large groups of topics &mdash; let's call them \"super-topics\" &mdash; one in the upper-right quadrant and the other in the lower-right quadrant. These super-topics correlate reasonably well with the pattern we'd noticed while naming the topics:\n",
    "* The super-topic in the *lower*-right tends to be about *food*. It groups together the **burger & fries**, **breakfast**, **sushi**, **barbecue**, and **greek** topics, among others.\n",
    "* The super-topic in the *upper*-right tends to be about other elements of the *restaurant experience*. It groups together the **ambience & seating**, **location & time**, **family**, and **customer service** topics, among others.\n",
    "\n",
    "So, in addition to the 50 direct topics the model has learned, our analysis suggests a higher-level pattern in the data. Restaurant reviewers in the Yelp dataset talk about two main things in their reviews, in general: (1) the food, and (2) their overall restaurant experience. For this dataset, this is a very intuitive result, and we probably didn't need a sophisticated modeling technique to tell it to us. When working with datasets from other domains, though, such high-level patterns may be much less obvious from the outset &mdash; and that's where topic modeling can help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### \u2705 Key Takeaways - Topic Modeling:\n",
    "- **LDA discovers themes** automatically from text without labels\n",
    "- **50 topics** capture food types (mexican, italian) and experiences (service, ambience)\n",
    "- **Bag-of-words** representation loses word order but captures content\n",
    "- **pyLDAvis** provides interactive exploration of topic relationships\n",
    "\n",
    "\ud83d\udca1 **Why it matters:** Understand millions of reviews at a glance by grouping similar themes!\n",
    "\n",
    "**Real-World Applications:**\n",
    "- \ud83d\udcf0 News article clustering and recommendation\n",
    "- \ud83d\udecd\ufe0f Product review summarization\n",
    "- \ud83d\udd0d Document search and organization\n",
    "- \ud83d\udcca Customer feedback analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing text with LDA\n",
    "Beyond data exploration, one of the key uses for an LDA model is providing a compact, quantitative description of natural language text. Once an LDA model has been trained, it can be used to represent free text as a mixture of the topics the model learned from the original corpus. This mixture can be interpreted as a probability distribution across the topics, so the LDA representation of a paragraph of text might look like 50% _Topic A_, 20% _Topic B_, 20% _Topic C_, and 10% _Topic D_.\n",
    "\n",
    "To use an LDA model to generate a vector representation of new text, you'll need to apply any text preprocessing steps you used on the model's training corpus to the new text, too. For our model, the preprocessing steps we used include:\n",
    "1. Using spaCy to remove punctuation and lemmatize the text\n",
    "1. Applying our first-order phrase model to join word pairs\n",
    "1. Applying our second-order phrase model to join longer phrases\n",
    "1. Removing stopwords\n",
    "1. Creating a bag-of-words representation\n",
    "\n",
    "Once you've applied these preprocessing steps to the new text, it's ready to pass directly to the model to create an LDA representation. The `lda_description(...)` function will perform all these steps for us, including printing the resulting topical description of the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_review(review_number):\n",
    "    \"\"\"\n",
    "    retrieve a particular review index\n",
    "    from the reviews file and return it\n",
    "    \"\"\"\n",
    "    \n",
    "    return list(it.islice(line_review(review_txt_filepath),\n",
    "                          review_number, review_number+1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_description(review_text, min_topic_freq=0.05):\n",
    "    \"\"\"\n",
    "    accept the original text of a review and (1) parse it with spaCy,\n",
    "    (2) apply text pre-proccessing steps, (3) create a bag-of-words\n",
    "    representation, (4) create an LDA representation, and\n",
    "    (5) print a sorted list of the top topics in the LDA representation\n",
    "    \"\"\"\n",
    "    \n",
    "    # parse the review text with spaCy\n",
    "    parsed_review = nlp(review_text)\n",
    "    \n",
    "    # lemmatize the text and remove punctuation and whitespace\n",
    "            # Step 1: Lemmatize and remove punctuation\n",
    "    unigram_review = [token.lemma_ for token in parsed_review\n",
    "                      if not punct_space(token)]\n",
    "    \n",
    "    # apply the first-order and secord-order phrase models\n",
    "            # Step 2: Join two-word phrases (e.g., 'ice cream' -> 'ice_cream')\n",
    "    bigram_review = bigram_model[unigram_review]\n",
    "            # Step 3: Join three-word phrases (e.g., 'vanilla ice_cream' -> 'vanilla_ice_cream')\n",
    "    trigram_review = trigram_model[bigram_review]\n",
    "    \n",
    "    # remove any remaining stopwords\n",
    "            # Step 4: Remove stopwords (common words like 'the', 'and')\n",
    "    trigram_review = [term for term in trigram_review\n",
    "                      if not term in spacy.lang.en.stop_words.STOP_WORDS]\n",
    "    \n",
    "    # create a bag-of-words representation\n",
    "    review_bow = trigram_dictionary.doc2bow(trigram_review)\n",
    "    \n",
    "    # create an LDA representation\n",
    "    review_lda = lda[review_bow]\n",
    "   \n",
    "    # Sort topics by frequency (highest first)\n",
    "    review_lda = sorted(review_lda, key=lambda x: -x[1])\n",
    "    \n",
    "    for topic_number, freq in review_lda:\n",
    "        if freq < min_topic_freq:\n",
    "            break\n",
    "            \n",
    "        # print the most highly related topic names and frequencies\n",
    "        print(f'{topic_names[topic_number]:25} {round(freq, 3)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_review = get_sample_review(50)\n",
    "print(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_description(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_review = get_sample_review(100)\n",
    "print(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_description(sample_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83c\udf0d Real-World Applications: Word Embeddings\n",
    "\n",
    "Word embeddings like Word2Vec power many real-world applications:\n",
    "\n",
    "**Search & Information Retrieval:**\n",
    "- **Google Search**: Uses BERT (based on word embeddings) to understand search intent\n",
    "  - Query \"how to fix slow computer\" matches documents about \"speed up PC performance\"\n",
    "  - Understands synonyms and related concepts without exact keyword matches\n",
    "\n",
    "**Recommendation Systems:**\n",
    "- **Amazon**: \"Customers who viewed this item also viewed...\"\n",
    "  - Uses product embeddings (similar to word embeddings) to find related products\n",
    "  - \"laptop\" vectors are close to \"mouse\", \"keyboard\", \"laptop_bag\" vectors\n",
    "- **Spotify**: Recommends songs by learning music embeddings from listening patterns\n",
    "\n",
    "**Content Moderation:**\n",
    "- **Facebook/YouTube**: Detect toxic content and hate speech\n",
    "  - Word embeddings help identify offensive terms and their variations\n",
    "  - Can catch misspellings, slang, and code-switching\n",
    "\n",
    "**Question Answering:**\n",
    "- **ChatGPT/Alexa**: Use transformer models (evolution of Word2Vec)\n",
    "  - Understand context and semantic meaning\n",
    "  - Generate human-like responses\n",
    "\n",
    "**For restaurants specifically:**\n",
    "- **Yelp/Google Reviews**: Categorize reviews by topic (food, service, ambiance)\n",
    "- **OpenTable**: Suggest restaurants based on review similarity\n",
    "- **Food delivery apps**: Understand \"spicy vegan noodles\" \u2248 \"hot plant-based ramen\"\n",
    "\n",
    "The technique you're learning here is a fundamental building block of modern NLP!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\uddee Part 5: Word Vector Embedding with Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### \ud83c\udfaf Learning Objectives:\n",
    "- Understand word vector embeddings\n",
    "- Learn how Word2Vec captures semantic meaning\n",
    "- Perform word algebra (breakfast + lunch = brunch)\n",
    "- Explore vector space with similarity queries\n",
    "\n",
    "**Time:** ~25 minutes | **Key Concept:** Dense vector representations of meaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![word2vec quiz 2](https://s3.amazonaws.com/skipgram-images/word2vec-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of *word vector embedding models*, or *word vector models* for short, is to learn dense, numerical vector representations for each term in a corpus vocabulary. If the model is successful, the vectors it learns about each term should encode some information about the *meaning* or *concept* the term represents, and the relationship between it and other terms in the vocabulary. Word vector models are also fully unsupervised &mdash they learn all of these meanings and relationships solely by analyzing the text of the corpus, without any advance knowledge provided.\n",
    "\n",
    "Perhaps the best-known word vector model is [word2vec](https://arxiv.org/pdf/1301.3781v3.pdf), originally proposed in 2013. The general idea of word2vec is, for a given *focus word*, to use the *context* of the word &mdash; i.e., the other words immediately before and after it &mdash; to provide hints about what the focus word might mean. To do this, word2vec uses a *sliding window* technique, where it considers snippets of text only a few tokens long at a time.\n",
    "\n",
    "At the start of the learning process, the model initializes random vectors for all terms in the corpus vocabulary. The model then slides the window across every snippet of text in the corpus, with each word taking turns as the focus word. Each time the model considers a new snippet, it tries to learn some information about the focus word based on the surrouding context, and it \"nudges\" the words' vector representations accordingly. One complete pass sliding the window across all of the corpus text is known as a training *epoch*. It's common to train a word2vec model for multiple passes/epochs over the corpus. Over time, the model rearranges the terms' vector representations such that terms that frequently appear in similar contexts have vector representations that are *close* to each other in vector space.\n",
    "\n",
    "For a deeper dive into word2vec's machine learning process, see [here](https://arxiv.org/pdf/1411.2738v4.pdf).\n",
    "\n",
    "Word2vec has a number of user-defined hyperparameters, including:\n",
    "- The dimensionality of the vectors. Typical choices include a few dozen to several hundred.\n",
    "- The width of the sliding window, in tokens. Five is a common default choice, but narrower and wider windows are possible.\n",
    "- The number of training epochs.\n",
    "\n",
    "For using word2vec in Python, [gensim](https://rare-technologies.com/deep-learning-with-word2vec-and-gensim/) comes to the rescue again! It offers a [highly-optimized](https://rare-technologies.com/word2vec-in-python-part-two-optimizing/), [parallelized](https://rare-technologies.com/parallelizing-word2vec-in-python/) implementation of the word2vec algorithm with its [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "trigram_sentences = LineSentence(trigram_sentences_filepath)\n",
    "word2vec_filepath = os.path.join(intermediate_directory, 'word2vec_model_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train our word2vec model using the normalized sentences with our phrase models applied. We'll use 100-dimensional vectors, and set up our training process to run for twelve epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to train the word2vec model yourself.\n",
    "if RECOMPUTE_DATA:\n",
    "    print('Training Word2Vec model...')\n",
    "    print('Learning 100-dimensional vectors for ~50,000 words')\n",
    "    print('Expected time: 2-3 minutes')\n",
    "\n",
    "    # WHY vector_size=100:\n",
    "    # - 50 dimensions: Too small, loses semantic nuances\n",
    "    # - 300 dimensions: Industry standard for general text, but overkill for domain-specific (restaurant) text\n",
    "    # - 100 dimensions: Sweet spot for this dataset - captures semantic relationships without overfitting\n",
    "    #\n",
    "    # WHY window=5:\n",
    "    # - Context window: how many words before/after to consider\n",
    "    # - window=2: Too narrow, misses important relationships\n",
    "    # - window=10: Too wide, includes unrelated words, adds noise\n",
    "    # - window=5: Standard choice, captures phrase-level context (e.g., \"delicious homemade apple pie\")\n",
    "    #\n",
    "    # WHY min_count=20:\n",
    "    # - Ignore words appearing fewer than 20 times\n",
    "    # - Rare words don't have enough context to learn good vectors\n",
    "    # - With 4.2M reviews, min_count=20 filters typos and extreme outliers\n",
    "    # - Keeps vocabulary at manageable ~50k terms instead of 200k+\n",
    "    #\n",
    "    # WHY sg=1 (skip-gram, not CBOW):\n",
    "    # - sg=0 (CBOW): Predicts word from context, faster training\n",
    "    # - sg=1 (skip-gram): Predicts context from word, better for rare words and phrases\n",
    "    # - Skip-gram works better for phrase-heavy text like \"chocolate_chip_cookie\"\n",
    "    #\n",
    "    # WHY workers=4:\n",
    "    # - Parallelize training across 4 CPU cores\n",
    "    # - Training takes 2-3 minutes instead of 8-12 minutes\n",
    "\n",
    "    # initiate the model and perform the first epoch of training\n",
    "    food2vec = Word2Vec(trigram_sentences, vector_size=100, window=5,\n",
    "                        min_count=20, sg=1, workers=4)\n",
    "    \n",
    "    food2vec.save(word2vec_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \u2705 Data Quality Check: Word2Vec Model Statistics\n",
    "\n",
    "Let's verify our Word2Vec model learned good word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec model quality metrics\n",
    "print(f'\ud83d\udcca Word2Vec Model Statistics:')\n",
    "print(f'  Vocabulary size: {len(food2vec.wv):,} terms')\n",
    "print(f'  Vector dimensions: {food2vec.wv.vector_size}')\n",
    "print(f'  Training epochs completed: {food2vec.epochs}')\n",
    "print(f'  Window size: {food2vec.window}')\n",
    "\n",
    "# Quick sanity check: similar words to \"pizza\"\n",
    "print(f'\\n  Sanity check - Words similar to \"pizza\":')\n",
    "try:\n",
    "    similar = food2vec.wv.most_similar('pizza', topn=5)\n",
    "    for word, similarity in similar:\n",
    "        print(f'    - {word:20s} (similarity: {similarity:.3f})')\n",
    "    print(f'\\n\u2705 Word2Vec model looks good! Semantically related words cluster together.')\n",
    "except KeyError:\n",
    "    print(f'  \"pizza\" not in vocabulary (appears fewer than {food2vec.min_count} times)')\n",
    "    print(f'  Try another common word instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if RECOMPUTE_DATA:\n",
    "    token_count = sum([len(sentence) for sentence in trigram_sentences])\n",
    "    # perform another 11 epochs of training\n",
    "    for i in range(1,12):\n",
    "\n",
    "        food2vec.train(trigram_sentences, total_examples= token_count, epochs=food2vec.epochs)\n",
    "        food2vec.save(word2vec_filepath)\n",
    "        print(f'{food2vec.train_count} training epochs so far.')\n",
    "        \n",
    "# load the finished model from disk\n",
    "food2vec = Word2Vec.load(word2vec_filepath)\n",
    "# Note: init_sims() is not needed in gensim 4.0+ (automatic optimization)\n",
    "\n",
    "print(f'{food2vec.train_count} training epochs so far.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('{:,} terms in the food2vec vocabulary.'.format(len(food2vec.wv.vocab)))\n",
    "print(f\"{len(food2vec.wv):,} terms in the food2vec vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the word vectors our model has learned. We'll create a pandas DataFrame with the terms as the row labels, and the 100 dimensions of the word vector model as the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a list of (term, index, count) tuples from the vocabulary\n",
    "ordered_vocab = []\n",
    "for term in food2vec.wv.index_to_key:  # Terms in frequency-descending order\n",
    "    ordered_vocab.append((\n",
    "        term,\n",
    "        food2vec.wv.key_to_index[term],            # index in the model\n",
    "        food2vec.wv.get_vecattr(term, \"count\")    # frequency count\n",
    "    ))\n",
    "\n",
    "# Sort by term counts (most common first)\n",
    "ordered_vocab = sorted(ordered_vocab, key=lambda x: -x[2])\n",
    "\n",
    "# Unpack into separate lists for easier use\n",
    "ordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n",
    "\n",
    "# Create DataFrame with normalized word vectors as data\n",
    "# Row labels are terms, columns are the 100 vector dimensions\n",
    "word_vectors = pd.DataFrame(\n",
    "    food2vec.wv.get_normed_vectors()[list(term_indices), :],\n",
    "    index=ordered_terms\n",
    ")\n",
    "\n",
    "word_vectors.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holy wall of numbers! This DataFrame has 50,835 rows &mdash; one for each term in the vocabulary &mdash; and 100 colums. Our model has learned a quantitative vector representation for each term, as expected.\n",
    "\n",
    "Put another way, our model has \"embedded\" the terms into a 100-dimensional vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So... what can we do with all these numbers?\n",
    "The first thing we can use them for is to simply look up related words and phrases for a given term of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_terms(token, topn=10):\n",
    "    \"\"\"\n",
    "    look up the topn most similar terms to token\n",
    "    and print them as a formatted list\n",
    "    \"\"\"\n",
    "\n",
    "    for word, similarity in food2vec.wv.most_similar(positive=[token], topn=topn):\n",
    "\n",
    "        print(f'{word:20} {round(similarity, 3)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What things are like Burger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_related_terms('burger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has learned that fast food restaurants are similar to each other! In particular, *mcdonalds* and *wendy's* are the most similar to Burger King, according to this dataset. In addition, the model has found that alternate spellings for the same entities are probably related, such as *mcdonalds*, *mcdonald's* and *mcd's*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When is happy hour?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_related_terms('happy_hour')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has noticed several alternate spellings for happy hour, such as *hh* and *happy hr*, and assesses them as highly related. If you were looking for reviews about happy hour, such alternate spellings would be very helpful to know.\n",
    "\n",
    "Taking a deeper look &mdash; the model has turned up phrases like *3-6pm*, *4-7pm*, and *mon-fri*, too. This is especially interesting, because the model has no advance knowledge at all about what happy hour is, and what time of day it should be. But simply by scanning through restaurant reviews, the model has discovered that the concept of happy hour has something very important to do with that block of time around 3-7pm on weekdays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make pasta tonight. Which style do you want?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_related_terms('pasta', topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word algebra!\n",
    "No self-respecting word2vec demo would be complete without a healthy dose of *word algebra*, also known as *analogy completion*.\n",
    "\n",
    "The core idea is that once words are represented as numerical vectors, you can do math with them. The mathematical procedure goes like this:\n",
    "1. Provide a set of words or phrases that you'd like to add or subtract.\n",
    "1. Look up the vectors that represent those terms in the word vector model.\n",
    "1. Add and subtract those vectors to produce a new, combined vector.\n",
    "1. Look up the most similar vector(s) to this new, combined vector via cosine similarity.\n",
    "1. Return the word(s) associated with the similar vector(s).\n",
    "\n",
    "But more generally, you can think of the vectors that represent each word as encoding some information about the *meaning* or *concepts* of the word. What happens when you ask the model to combine the meaning and concepts of words in new ways? Let's see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_algebra(add=[], subtract=[], topn=2):\n",
    "    \"\"\"\n",
    "    Perform word algebra by combining word vectors, then find similar words.\n",
    "    \n",
    "    How it works:\n",
    "    1. Look up the 100-dimensional vector for each word in add= and subtract=\n",
    "    2. Combine them: result_vector = sum(add vectors) - sum(subtract vectors)\n",
    "    3. Compare this result_vector to EVERY word in the vocabulary\n",
    "    4. Return the words whose vectors are most similar to result_vector\n",
    "    \n",
    "    The similarity score is COSINE SIMILARITY comparing:\n",
    "      - The combined result_vector (from step 2)\n",
    "      - Each candidate word's vector (from the vocabulary)\n",
    "    \n",
    "    Similarity interpretation:\n",
    "    - 1.0 = candidate vector points exactly the same direction as result_vector\n",
    "    - 0.0 = candidate vector is perpendicular to result_vector (unrelated)\n",
    "    - Typical values: 0.3-0.9 for food/restaurant terms\n",
    "    \n",
    "    Example: word_algebra(add=['breakfast', 'lunch'])\n",
    "      \u2192 Creates vector by adding breakfast_vector + lunch_vector\n",
    "      \u2192 Finds words whose vectors are closest to this combined vector\n",
    "      \u2192 Returns 'brunch' (high similarity ~0.85) because brunch_vector \u2248 breakfast_vector + lunch_vector\n",
    "    \"\"\"\n",
    "    answers = food2vec.wv.most_similar(positive=add, negative=subtract, topn=topn)\n",
    "    \n",
    "    for term, similarity in answers:\n",
    "        print(f'{term:20s} (similarity: {similarity:.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### breakfast + lunch = ?\n",
    "Let's start with a softball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=['breakfast', 'lunch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so the model knows that *brunch* is a combination of *breakfast* and *lunch*. What else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lunch - day + night = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=['lunch', 'night'], subtract=['day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're getting a bit more nuanced. The model has discovered that:\n",
    "- Both *lunch* and *dinner* are meals\n",
    "- The main difference between them is time of day\n",
    "- Day and night are times of day\n",
    "- Lunch is associated with day, and dinner is associated with night\n",
    "\n",
    "What else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### taco - mexican + chinese = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=['taco', 'chinese'], subtract=['mexican'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an entirely new and different type of relationship that the model has learned.\n",
    "- It knows that tacos are a characteristic example of Mexican food\n",
    "- It knows that Mexican and Chinese are both styles of food\n",
    "- If you subtract *Mexican* from *taco*, you're left with something like the concept of a *\"characteristic type of food\"*, which is represented as a new vector\n",
    "- If you add that new *\"characteristic type of food\"* vector to Chinese, you get *dumpling*.\n",
    "\n",
    "What else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bun - american + mexican = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=['bun', 'mexican'], subtract=['american'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model knows that both *buns* and *tortillas* are the doughy thing that goes on the outside of your real food, and that the primary difference between them is the style of food they're associated with.\n",
    "\n",
    "What else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filet mignon - beef + seafood = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=['filet_mignon', 'seafood'], subtract=['beef'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has learned a concept of *delicacy*. If you take filet mignon and subtract beef from it, you're left with a vector that roughly corresponds to delicacy. If you add the delicacy vector to *seafood*, you get *raw oyster*.\n",
    "\n",
    "What else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coffee - drink + snack = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=['coffee', 'snack'], subtract=['drink'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model knows that if you're on your coffee break, but instead of drinking something, you're eating something... that thing is most likely a pastry.\n",
    "\n",
    "What else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a good place to land... what if we explore the vector space around *Applebee's* a bit, in a few different directions? Let's see what we find.\n",
    "\n",
    "#### Applebee's + italian = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_algebra(add=[\"applebee_'s\", 'italian'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applebee's + pancakes = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=[\"applebee_'s\", 'pancakes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applebee's + pizza = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=[\"applebee_'s\", 'pizza'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could do this all day. One last analogy before we move on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wine - grapes + barley = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_algebra(add=['wine', 'barley'], subtract=['grapes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udd2c More Word Algebra Examples: Diverse Semantic Relationships\n",
    "\n",
    "Let's explore different types of semantic transformations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Cuisine Style Transfer**\n",
    "\n",
    "What if we take Italian cuisine and make it spicy (like Thai food)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# italian + spicy - mild = ?\n",
    "word_algebra(add=['italian', 'spicy'], subtract=['mild'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Dietary Substitutions**\n",
    "\n",
    "What's the vegetarian version of a burger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# burger + vegetarian - meat = ?\n",
    "word_algebra(add=['burger', 'vegetarian'], subtract=['meat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Meal Time Transformations**\n",
    "\n",
    "What happens when we move pizza from dinner to breakfast?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pizza + breakfast - dinner = ?\n",
    "word_algebra(add=['pizza', 'breakfast'], subtract=['dinner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Texture Transformations**\n",
    "\n",
    "What's the crispy version of chicken (vs. baked)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chicken + crispy - baked = ?\n",
    "word_algebra(add=['chicken', 'crispy'], subtract=['baked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Preparation Method Analogies**\n",
    "\n",
    "If we take fish and prepare it Japanese-style (instead of American-style)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fish + japanese - american = ?\n",
    "word_algebra(add=['fish', 'japanese'], subtract=['american'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Atmosphere Transfer**\n",
    "\n",
    "What's the outdoor equivalent of brunch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brunch + outdoor - indoor = ?\n",
    "word_algebra(add=['brunch', 'outdoor'], subtract=['indoor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcc9 Part 6: Word Vector Visualization with t-SNE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### \ud83c\udfaf Learning Objectives:\n",
    "- Understand dimensionality reduction with t-SNE\n",
    "- Visualize 100-dimensional vectors in 2D space\n",
    "- Create interactive plots with Bokeh\n",
    "- Explore semantic relationships visually\n",
    "\n",
    "**Time:** ~10 minutes | **Key Concept:** Visualizing high-dimensional data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[t-Distributed Stochastic Neighbor Embedding](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf), or *t-SNE* for short, is a dimensionality reduction technique to assist with visualizing high-dimensional datasets. It attempts to map high-dimensional data onto a low two- or three-dimensional representation such that the relative distances between points are preserved as closely as possible in both high-dimensional and low-dimensional space.\n",
    "\n",
    "scikit-learn provides a convenient implementation of the t-SNE algorithm with its [TSNE](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input for t-SNE will be the DataFrame of word vectors we created before. Let's first:\n",
    "1. Drop stopwords &mdash; it's probably not too interesting to visualize *the*, *of*, *or*, and so on\n",
    "1. Take only the 5,000 most frequent terms in the vocabulary &mdash; no need to visualize all ~50,000 terms right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit to top N words for visualization performance\n",
    "TOP_N_WORDS_FOR_VIZ = 5000\n",
    "\n",
    "tsne_input = word_vectors.drop(spacy.lang.en.stop_words.STOP_WORDS, errors='ignore')\n",
    "tsne_input = tsne_input.head(TOP_N_WORDS_FOR_VIZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_filepath = os.path.join(intermediate_directory,\n",
    "                             'tsne_model')\n",
    "\n",
    "tsne_vectors_filepath = os.path.join(intermediate_directory,\n",
    "                                     'tsne_vectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import pickle\n",
    "\n",
    "# Comprehensive pandas compatibility fix for loading old pickled files\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pandas import Index\n",
    "\n",
    "# Map old pandas modules\n",
    "if 'pandas.core.indexes.numeric' not in sys.modules:\n",
    "    import pandas.core.indexes.api as idx_api\n",
    "    sys.modules['pandas.core.indexes.numeric'] = idx_api\n",
    "if 'pandas.indexes' not in sys.modules:\n",
    "    sys.modules['pandas.indexes'] = pd.core.indexes.api\n",
    "\n",
    "# Map old Index types\n",
    "pd.Int64Index = Index\n",
    "pd.core.indexes.api.Int64Index = Index\n",
    "pd.Float64Index = Index\n",
    "pd.core.indexes.api.Float64Index = Index\n",
    "\n",
    "if RECOMPUTE_DATA:\n",
    "    # WHY use t-SNE:\n",
    "    # - Can't visualize 100-dimensional vectors directly\n",
    "    # - Need to reduce to 2D for plotting\n",
    "    # - PCA: Linear reduction, fast but misses nonlinear relationships\n",
    "    # - t-SNE: Nonlinear reduction, preserves local neighborhoods\n",
    "    #   (words with similar meanings stay close together in 2D)\n",
    "    #\n",
    "    # WHY default parameters work well:\n",
    "    # - perplexity=30 (default): Balances local vs global structure\n",
    "    # - n_iter=1000 (default): Enough iterations for convergence\n",
    "    # - learning_rate='auto': Adaptive learning for stable convergence\n",
    "    #\n",
    "    # WHY limit to 5,000 words (from tsne_input):\n",
    "    # - t-SNE is O(n\u00b2) - computing on 50k words would take hours\n",
    "    # - 5,000 most common words covers key vocabulary\n",
    "    # - Still shows meaningful clusters and relationships\n",
    "    \n",
    "    print('Computing t-SNE projection of 5,000 word vectors...')\n",
    "    print('Reducing from 100 dimensions to 2 dimensions')\n",
    "    print('This takes 30-60 seconds...')\n",
    "    tsne = TSNE()\n",
    "    tsne_vectors = tsne.fit_transform(tsne_input.values)\n",
    "    \n",
    "    with open(tsne_filepath, 'wb') as f:\n",
    "        pickle.dump(tsne, f)\n",
    "    \n",
    "    np.save(tsne_vectors_filepath, tsne_vectors)\n",
    "\n",
    "with open(tsne_filepath, 'rb') as f:\n",
    "    tsne = pickle.load(f)\n",
    "\n",
    "tsne_vectors = np.load(tsne_vectors_filepath)\n",
    "\n",
    "tsne_vectors = pd.DataFrame(tsne_vectors,\n",
    "                            index=pd.Index(tsne_input.index),\n",
    "                            columns=['x_coord', 'y_coord'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a two-dimensional representation of our data! Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_vectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_vectors['word'] = tsne_vectors.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting with Bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource\n",
    "from bokeh.core.properties import value  # \u2190 moved here\n",
    "from bokeh.models import WheelZoomTool\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = ColumnDataSource(tsne_vectors)\n",
    "\n",
    "tsne_plot = figure(\n",
    "    title='t-SNE Word Embeddings',\n",
    "    width=800,\n",
    "    height=800,\n",
    "    tools=\"pan,wheel_zoom,box_zoom,box_select,reset\",\n",
    ")\n",
    "\n",
    "# Make wheel zoom the active scroll tool\n",
    "wheel = tsne_plot.select_one(WheelZoomTool)\n",
    "tsne_plot.toolbar.active_scroll = wheel\n",
    "\n",
    "tsne_plot.scatter(\n",
    "    x='x_coord', y='y_coord',\n",
    "    source=plot_data,\n",
    "    size=10, line_alpha=0.2, fill_alpha=0.1,\n",
    ")\n",
    "\n",
    "tsne_plot.add_tools(HoverTool(tooltips=[(\"word\", \"@index\")]))\n",
    "\n",
    "tsne_plot.title.text_font_size = \"16pt\"\n",
    "tsne_plot.xaxis.visible = False\n",
    "tsne_plot.yaxis.visible = False\n",
    "tsne_plot.grid.grid_line_color = None\n",
    "tsne_plot.outline_line_color = None\n",
    "\n",
    "show(tsne_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2753 Frequently Asked Questions\n",
    "\n",
    "### About the Data\n",
    "\n",
    "**Q: Why use Yelp restaurant reviews?**\n",
    "- Restaurant reviews are rich with domain-specific vocabulary\n",
    "- Clear semantic relationships (food, cuisines, experiences)\n",
    "- Large dataset (4.2M reviews) for robust model training\n",
    "- Relatable domain - everyone understands food!\n",
    "\n",
    "**Q: Can I use my own data?**\n",
    "- Yes! Replace `review.json` with your own text data\n",
    "- Format: One JSON object per line with a 'text' field\n",
    "- Minimum: ~100K documents recommended for good Word2Vec results\n",
    "- Adjust `min_count` parameters for smaller datasets\n",
    "\n",
    "### About Word2Vec\n",
    "\n",
    "**Q: Why 100 dimensions instead of 300?**\n",
    "- 300 is common for general text (trained on billions of words)\n",
    "- 100 is sufficient for domain-specific text (restaurant reviews)\n",
    "- Lower dimensions = faster training, less overfitting\n",
    "- You can experiment with `vector_size` parameter!\n",
    "\n",
    "**Q: What's the difference between CBOW and skip-gram?**\n",
    "- **CBOW** (`sg=0`): Predicts word from context, faster training\n",
    "- **Skip-gram** (`sg=1`): Predicts context from word, better for rare words\n",
    "- We use skip-gram because we have many phrases (trigrams)\n",
    "\n",
    "**Q: Can I update the model with new data?**\n",
    "- Yes! Use `food2vec.build_vocab(new_sentences, update=True)`\n",
    "- Then `food2vec.train(new_sentences, ...)`\n",
    "- Useful for incremental learning with new reviews\n",
    "\n",
    "### About LDA\n",
    "\n",
    "**Q: How do I choose the number of topics?**\n",
    "- No single \"correct\" number - it's a hyperparameter\n",
    "- Too few (10): Topics too broad, lack specificity\n",
    "- Too many (200): Topics too narrow, redundant\n",
    "- Rule of thumb: sqrt(vocabulary_size) as a starting point\n",
    "- Try 20-100 for most applications, evaluate interpretability\n",
    "\n",
    "**Q: Why do topic numbers differ between gensim and pyLDAvis?**\n",
    "- Different indexing/ordering systems\n",
    "- Match topics by top terms, not by numbers\n",
    "- This is a known limitation - they use the same underlying data\n",
    "\n",
    "**Q: Can I use LDA for prediction?**\n",
    "- LDA is primarily for exploration and understanding\n",
    "- Can use topic distributions as features for classification\n",
    "- Example: Predict restaurant rating from topic mixture\n",
    "\n",
    "### About Phrase Detection\n",
    "\n",
    "**Q: How does phrase detection work?**\n",
    "- Statistical approach: finds word pairs that appear together more often than chance\n",
    "- Uses pointwise mutual information (PMI) or similar metrics\n",
    "- `threshold` parameter controls how strict to be\n",
    "- Higher threshold = only obvious phrases (ice_cream)\n",
    "- Lower threshold = more phrases, some spurious\n",
    "\n",
    "**Q: Can I add custom phrases?**\n",
    "- Yes! Use Phraser with a custom dictionary\n",
    "- Or post-process to force certain phrases\n",
    "- Example: Always join \"San_Francisco\", \"New_York\"\n",
    "\n",
    "### About Performance\n",
    "\n",
    "**Q: Why is this so slow on my machine?**\n",
    "- Text processing is CPU-intensive (4.2M reviews!)\n",
    "- Use `RECOMPUTE_DATA = False` to skip expensive computations\n",
    "- Consider running on cloud (Google Colab, AWS, etc.)\n",
    "- Reduce dataset size for learning (sample 100K reviews)\n",
    "\n",
    "**Q: Can I use GPU acceleration?**\n",
    "- Gensim Word2Vec: CPU-optimized, GPU doesn't help much\n",
    "- LDA: CPU-based algorithm, no GPU version\n",
    "- For GPU: Use PyTorch or TensorFlow-based implementations\n",
    "\n",
    "### Learning More\n",
    "\n",
    "**Q: What should I learn next?**\n",
    "- **Modern transformers**: BERT, GPT, T5 (evolution of Word2Vec)\n",
    "- **Deep learning NLP**: PyTorch, Hugging Face Transformers\n",
    "- **Advanced topic modeling**: BERTopic, Top2Vec\n",
    "- **Production NLP**: spaCy pipelines, model deployment\n",
    "\n",
    "**Q: Where can I find more resources?**\n",
    "- **gensim tutorials**: radimrehurek.com/gensim/\n",
    "- **spaCy course**: course.spacy.io\n",
    "- **Papers**: Word2Vec original paper (Mikolov et al. 2013)\n",
    "- **Books**: \"Speech and Language Processing\" (Jurafsky & Martin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whew! Let's round up the major components that we've seen:\n",
    "1. Text processing with **spaCy**\n",
    "1. Automated **phrase modeling**\n",
    "1. Topic modeling with **LDA** $\\ \\longrightarrow\\ $ visualization with **pyLDAvis**\n",
    "1. Word vector modeling with **word2vec** $\\ \\longrightarrow\\ $ visualization with **t-SNE**\n",
    "\n",
    "#### Why use these models?\n",
    "Dense vector representations for text like LDA and word2vec can greatly improve performance for a number of common, text-heavy problems like:\n",
    "- Text classification\n",
    "- Search\n",
    "- Recommendations\n",
    "- Question answering\n",
    "\n",
    "...and more generally are a powerful way machines can help humans make sense of what's in a giant pile of text. They're also often useful as a pre-processing step for many other downstream machine learning applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}